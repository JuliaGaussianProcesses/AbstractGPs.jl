<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>One-dimensional regression · AbstractGPs.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body>
<!-- NAVBAR START -->
<style>
    html {
        scroll-padding-top: calc(55px + 1rem);
    }

    /* Documenter css tweaks */
    .docs-sidebar {
        margin-top: 3.75rem;
    }

    #documenter {
        margin-top: 3.75rem;
    }

    .docs-version-selector {
        margin-bottom: 60px !important;
    }

    @media screen and (max-width: 1056px) {
        .docs-version-selector {
            margin-bottom: 60px !important;
        }

        .docs-sidebar {
            margin-top: 0 !important;
        }
    }
    /* Documenter css tweaks ends here */

    :root {
        --heading-color: white;
        --item-color: rgb(165, 165, 165);
        --primary-bg: #073c44;
        --hover-color: #8faad2;
    }

    .ext-navigation {
        position: fixed;
        height: 3.75rem;
        top: 0;
        width: 100%;
        background-color: var(--primary-bg);
        z-index: 1000;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        display: flex;
        align-items: center;
        padding: 0 1.0625rem;
        transition: transform 0.3s;
    }

    .ext-navbar-logo {
        margin-left: 0.625rem;
    }

    .ext-nav-links {
        display: flex;
        align-items: center;
        list-style-type: none;
        margin: 0;
        padding: 0;
        flex-grow: 1;
    }

    .ext-nav-links li {
        margin-left: 1rem !important;
    }

    .ext-nav-link {
        color: white !important;
        text-decoration: none;
        font-size: 1.0625rem !important;
        transition: color 0.2s ease;
        cursor: pointer;
    }

    .ext-nav-link:hover,
    .ext-navbar-item-single a:hover {
        color: var(--hover-color) !important;
    }

    .ext-navbar-item-single a {
        color: #fff !important;
    }

    .ext-menu-toggle {
        display: none;
        font-size: 1.5rem;
        color: white;
        cursor: pointer;
    }

    .ext-dropdown {
        display: none;
        grid-template-columns: 1fr 1fr 1fr 1fr;
        grid-template-rows: auto auto auto;
        padding: 1.875rem;
        position: absolute;
        width: 100%;
        left: 0;
        background-color: #083c44;
        line-height: 1.875rem;
        opacity: 0;
        transition: opacity 0.3s ease-in-out, transform 0.3s ease-in-out;
        transform: translateY(-0.625rem);
    }

    #library-handler::after {
        content: "▼";
        font-size: 0.6875rem;
        margin-left: 0.3125rem;
        transition: transform 0.3s ease-in-out;
    }

    #library-handler.open::after {
        content: "▲";
    }

    .ext-dropdown.show {
        display: grid;
        opacity: 1;
        transform: translateY(0);
    }

    .ext-dropdown ul {
        height: auto;
        width: 12.5rem;
        margin-bottom: 1.25rem;
    }

    .ext-dropdown ul li {
        text-align: left;
    }

    .navbar-sub-item {
        list-style: none;
    }

    .ext-dropdown ul a li {
        color: var(--item-color);
        width: 15.625rem;
        border-radius: 3px;
        padding: 0.125rem 0.625rem;
        transition: background-color 0.2s ease;
    }

    .ext-dropdown ul a li:hover {
        background-color: rgba(107, 107, 107, 0.5);
    }

    .ext-dropdown-item-heading {
        color: var(--heading-color);
        text-align: center;
    }

    /* Responsive styling */
    @media (max-width: 966px) {
        .ext-dropdown {
            grid-template-columns: 1fr 1fr 1fr;
        }
    }

    @media (max-width: 768px) {
        .ext-nav-links {
            display: none;
            flex-direction: column;
            width: 100%;
            background-color: var(--primary-bg);
            position: absolute;
            top: 3.75rem;
            left: 0;
            padding: 0.625rem 0;
            height: auto;
            overflow-y: auto;
            scrollbar-width: thin;
            scrollbar-color: rgb(141, 141, 141) grey;
        }

        .ext-nav-links.show {
            display: flex;
        }

        .ext-nav-links li {
            margin: 0.625rem 0;
            text-align: center;
        }

        .ext-menu-toggle {
            display: block;
            margin-left: auto;
        }

        .ext-navigation.hide {
            transform: translateY(-3.75rem);
        }

        .ext-dropdown {
            place-content: center;
            text-align: center;
            grid-template-columns: 1fr;
            line-height: 1.25rem;
            padding: 0.625rem;
        }

        .ext-dropdown ul {
            width: 100%;
            text-align: center;
            margin-bottom: 0.3125rem;
        }

        .ext-dropdown ul li {
            text-align: center;
        }

        .ext-dropdown ul a li {
            width: 100%;
        }

        .ext-dropdown ul a li:hover {
            background-color: var(--primary-bg);
            color: #fff;
        }

        /* Modified scroll bar */
        .ext-nav-links::-webkit-scrollbar {
            width: 5px;
        }

        .ext-nav-links::-webkit-scrollbar-track {
            box-shadow: inset 0 0 5px grey;
        }

        .ext-nav-links::-webkit-scrollbar-thumb {
            background: rgb(141, 141, 141);
            border-radius: 3px;
        }

        .ext-nav-links::-webkit-scrollbar-thumb:hover {
            background: #9b9b9b;
        }
    }
    @media only screen and (max-width: 768px) {
        .turing-logo {
            display: none !important;
        }
    }
    @media only screen and (min-width: 768px) {
        .turing-collab {
            display: none !important;
        }
    }
</style>
<nav class="ext-navigation">
    <a href="https://github.com/JuliaGaussianProcesses">
        <img src="https://avatars.githubusercontent.com/u/57909728?s=200&v=4" alt="JuliaGP Logo" class="ext-navbar-logo" height="24px" width="40px">
    </a>
    <a style="color: white !important; font-size: 21.25px !important; margin-left: 10px;" href="https://github.com/JuliaGaussianProcesses">JuliaGP</a>
    <ul class="ext-nav-links">
        <li>
            <a class="ext-nav-link" href="https://juliagaussianprocesses.github.io/AbstractGPs.jl/">AbstractGPs</a>
        </li>
        <li>
            <a class="ext-nav-link" href="https://juliagaussianprocesses.github.io/KernelFunctions.jl/">KernelFunctions</a>
        </li>
        <li>
            <a class="ext-nav-link" href="https://juliagaussianprocesses.github.io/GPLikelihoods.jl/">GPLikelihoods</a>
        </li>
        <li>
            <a class="ext-nav-link" href="https://juliagaussianprocesses.github.io/ApproximateGPs.jl/">ApproximateGPs</a>
        </li>
        <li>
            <a class="ext-nav-link turing-collab" href="https://turinglang.org">Co-developed with Turing.jl</a>
        </li>
        <!-- Add a Dropdown with these classes in case it's required so that the current CSS works fine -->
        <!-- <li>
            <p class="ext-nav-link" id="library-handler">Libraries</p>
            <div class="ext-dropdown" id="ext-dropdown-items">
                <ul>
                    <li class="ext-dropdown-item-heading">Modellinglanguages</li>
                    <a href="https://turinglang.org/DynamicPPL.jl/">
                        <li>DynamicPPL</li>
                    </a>
                    <a href="https://turinglang.org/JuliaBUGS.jl/">
                        <li>JuliaBUGS</li>
                    </a>
                    <a href="https://turinglang.org/TuringGLM.jl/">
                        <li>TurineGLM</li>
                    </a>
                </ul>
                <ul>
                    <li class="ext-dropdown-item-heading">MCMC</li>
                    <a href="https://turinglang.org/AdvancedHMC.jl/">
                        <li>AdvancedHMC</li>
                    </a>
                    <a href="https://turinglang.org/AbstractMCMC.jl/">
                        <li>AbstractMCMC</li>
                    </a>
                    <a href="https://github.com/theogf/ThermodynamicIntegration.jl">
                        <li>ThermodynamicIntegration</li>
                    </a>
                    <a href="https://turinglang.org/AdvancedPS.jl/">
                        <li>AdvancedPS</li>
                    </a>
                    <a href="https://turinglang.org/EllipticalSliceSampling.jl/">
                        <li>EllipticalSliceSampling</li>
                    </a>
                    <a href="https://turinglang.org/NestedSamplers.jl/">
                        <li>NestedSamplers</li>
                    </a>
                </ul>
                <ul>
                    <li class="ext-dropdown-item-heading">Diagnostics</li>
                    <a href="https://turinglang.org/MCMCChains.jl/">
                        <li>MCMCChains</li>
                    </a>
                    <a href="https://turinglang.org/MCMCDiagnosticTools.jl/">
                        <li>MCMCDiagnosticTools</li>
                    </a>
                    <a href="https://turinglang.org/ParetoSmooth.jl/">
                        <li>ParetoSmooth</li>
                    </a>
                </ul>
                <ul>
                    <li class="ext-dropdown-item-heading">Gaussion Processes</li>
                    <a href="https://juliagaussianprocesses.github.io/AbstractGPs.jl/">
                        <li>AbstractGPs</li>
                    </a>
                    <a href="https://juliagaussianprocesses.github.io/KernelFunctions.jl/">
                        <li>KernelFunctions</li>
                    </a>
                    <a href="https://juliagaussianprocesses.github.io/ApproximateGPs.jl/">
                        <li>ApproximateGPs</li>
                    </a>
                </ul>
                <ul>
                    <li class="ext-dropdown-item-heading ext-navbar-item-single">
                        <a href="https://turinglang.org/Bijectors.jl/">Bijectors</a>
                    </li>
                </ul>
                <ul>
                    <li class="ext-dropdown-item-heading ext-navbar-item-single">
                        <a href="https://turinglang.org/TuringCallbacks.jl/">TuringCallbacks</a>
                    </li>
                </ul>
                <ul>
                    <li class="ext-dropdown-item-heading ext-navbar-item-single">
                        <a href="https://turinglang.org/TuringBenchmarking.jl/">TuringBenchmarking</a>
                    </li>
                </ul>
            </div>
        </li> -->
    </ul>
    <a href="https://turinglang.org/" title="Co-developed with Turing.jl">
        <img src="https://turinglang.org/assets/images/turing-logo.svg" alt="Turing Logo" class="ext-navbar-logo turing-logo" height="24px" width="40px">
    </a>
    <!-- Github Logo -->
    <!-- <a href="https://github.com/JuliaGaussianProcesses/">
        <svg width="32px" height="32px" viewBox="-8.2 -8.2 36.40 36.40" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" fill="#000000">
            <g id="SVGRepo_bgCarrier" stroke-width="0"></g>
            <g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g>
            <g id="SVGRepo_iconCarrier">
                <title>github [#142]</title>
                <desc>Created with Sketch.</desc>
                <defs></defs>
                <g id="Page-1" stroke-width="0.0002" fill="none" fill-rule="evenodd">
                    <g id="Dribbble-Light-Preview" transform="translate(-140.000000, -7559.000000)" fill="#ffffff">
                        <g id="icons" transform="translate(56.000000, 160.000000)">
                            <path
                                d="M94,7399 C99.523,7399 104,7403.59 104,7409.253 C104,7413.782 101.138,7417.624 97.167,7418.981 C96.66,7419.082 96.48,7418.762 96.48,7418.489 C96.48,7418.151 96.492,7417.047 96.492,7415.675 C96.492,7414.719 96.172,7414.095 95.813,7413.777 C98.04,7413.523 100.38,7412.656 100.38,7408.718 C100.38,7407.598 99.992,7406.684 99.35,7405.966 C99.454,7405.707 99.797,7404.664 99.252,7403.252 C99.252,7403.252 98.414,7402.977 96.505,7404.303 C95.706,7404.076 94.85,7403.962 94,7403.958 C93.15,7403.962 92.295,7404.076 91.497,7404.303 C89.586,7402.977 88.746,7403.252 88.746,7403.252 C88.203,7404.664 88.546,7405.707 88.649,7405.966 C88.01,7406.684 87.619,7407.598 87.619,7408.718 C87.619,7412.646 89.954,7413.526 92.175,7413.785 C91.889,7414.041 91.63,7414.493 91.54,7415.156 C90.97,7415.418 89.522,7415.871 88.63,7414.304 C88.63,7414.304 88.101,7413.319 87.097,7413.247 C87.097,7413.247 86.122,7413.234 87.029,7413.87 C87.029,7413.87 87.684,7414.185 88.139,7415.37 C88.139,7415.37 88.726,7417.2 91.508,7416.58 C91.513,7417.437 91.522,7418.245 91.522,7418.489 C91.522,7418.76 91.338,7419.077 90.839,7418.982 C86.865,7417.627 84,7413.783 84,7409.253 C84,7403.59 88.478,7399 94,7399"
                                id="github-[#142]">
                            </path>
                        </g>
                    </g>
                </g>
            </g>
        </svg>
    </a> -->
    <span class="ext-menu-toggle">&#9776;</span>
</nav>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        const menuToggle = document.querySelector(".ext-menu-toggle");
        const navLinks = document.querySelector(".ext-nav-links");
        const nav = document.querySelector(".ext-navigation");
        const navigationHandler = document.getElementById("library-handler");
        const navigationItemsContainer =
            document.getElementById("ext-dropdown-items");
        let lastScrollY = window.scrollY;

        function setAppropriateHeight() {
            if (window.innerWidth <= 768) {
                const viewportHeight = window.innerHeight;
                const navHeight = nav.offsetHeight;
                navLinks.style.maxHeight = `${viewportHeight - navHeight}px`;
                navLinks.style.overflowY = "auto";
            } else {
                navLinks.style.maxHeight = "";
                navLinks.style.overflowY = "";
            }
        }

        // Toggle main menu for mobile
        menuToggle.addEventListener("click", () => {
            navLinks.classList.toggle("show");
            if (navLinks.classList.contains("show")) {
                setAppropriateHeight();
                // Ensure the dropdown is hidden when menu is first opened
                navigationItemsContainer.style.display = "none";
                navigationItemsContainer.classList.remove("show");
            }
        });

        // Close menus if clicked outside
        document.addEventListener("click", (event) => {
            if (
                !navLinks.contains(event.target) &&
                !menuToggle.contains(event.target)
            ) {
                navLinks.classList.remove("show");
                navigationItemsContainer.classList.remove("show");
                navigationHandler.classList.remove("open");
            }
        });

        // Hide navigation bar on scroll down in mobile view
        window.addEventListener("scroll", () => {
            if (window.innerWidth <= 768) {
                nav.classList.toggle("hide", window.scrollY > lastScrollY);
                lastScrollY = window.scrollY;
            }
        });

        // Library API script
        navigationHandler.addEventListener("click", (event) => {
            event.preventDefault(); // Prevent default action of the link
            if (navigationItemsContainer.classList.contains("show")) {
                navigationItemsContainer.classList.remove("show");
                navigationHandler.classList.remove("open");
                setTimeout(() => {
                    navigationItemsContainer.style.display = "none";
                }, 500); // Match the timeout to the CSS transition duration
            } else {
                navigationItemsContainer.style.display = "grid";
                navigationHandler.classList.add("open");
                setTimeout(() => {
                    navigationItemsContainer.classList.add("show");
                }, 10); // Delay to ensure the display change takes effect before adding class
            }
            setAppropriateHeight(); // Recalculate height when dropdown changes
        });

        // Handle window resize
        window.addEventListener("resize", setAppropriateHeight);

        // Initial setup
        setAppropriateHeight();
    });
</script>
<!-- NAVBAR END -->

<div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">AbstractGPs.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../api/">The Main APIs</a></li><li><a class="tocitem" href="../../concrete_features/">Concrete Features</a></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>One-dimensional regression</a><ul class="internal"><li><a class="tocitem" href="#Setup"><span>Setup</span></a></li><li><a class="tocitem" href="#Markov-Chain-Monte-Carlo"><span>Markov Chain Monte Carlo</span></a></li><li><a class="tocitem" href="#Variational-Inference"><span>Variational Inference</span></a></li><li><a class="tocitem" href="#Exact-Gaussian-Process-Inference"><span>Exact Gaussian Process Inference</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>One-dimensional regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>One-dimensional regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/master/examples/regression-1d/script.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="One-dimensional-regression"><a class="docs-heading-anchor" href="#One-dimensional-regression">One-dimensional regression</a><a id="One-dimensional-regression-1"></a><a class="docs-heading-anchor-permalink" href="#One-dimensional-regression" title="Permalink"></a></h1><p><a href="https://nbviewer.jupyter.org/github/JuliaGaussianProcesses/AbstractGPs.jl/blob/gh-pages/v0.3.10/examples/regression-1d.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-579ACA.svg" alt/></a></p><p><em>You are seeing the HTML output generated by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a> from the <a href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/master/examples/regression-1d/script.jl">Julia source file</a>. The corresponding notebook can be viewed in <a href="https://nbviewer.jupyter.org/github/JuliaGaussianProcesses/AbstractGPs.jl/blob/gh-pages/v0.3.10/examples/regression-1d.ipynb">nbviewer</a>.</em></p><h2 id="Setup"><a class="docs-heading-anchor" href="#Setup">Setup</a><a id="Setup-1"></a><a class="docs-heading-anchor-permalink" href="#Setup" title="Permalink"></a></h2><p>Loading the necessary packages and setting seed.</p><pre><code class="language-julia hljs">using AbstractGPs
using Distributions
using StatsFuns

using Plots
default(; legend=:outertopright, size=(700, 400))

using Random
Random.seed!(1234)</code></pre><p>Load toy regression <a href="https://github.com/GPflow/docs/blob/master/doc/source/notebooks/basics/data/regression_1D.csv">dataset</a> taken from GPFlow examples.</p><pre><code class="language-julia hljs">x = [
    0.8658165855998895,
    0.6661700880180962,
    0.8049218148148531,
    0.7714303440386239,
    0.14790478354654835,
    0.8666105548197428,
    0.007044577166530286,
    0.026331737288148638,
    0.17188596617099916,
    0.8897812990554013,
    0.24323574561119998,
    0.028590102134105955,
]
y = [
    1.5255314337144372,
    3.6434202968230003,
    3.010885733911661,
    3.774442382979625,
    3.3687639483798324,
    1.5506452040608503,
    3.790447985799683,
    3.8689707574953,
    3.4933565751758713,
    1.4284538820635841,
    3.8715350915692364,
    3.7045949061144983,
]
scatter(x, y; xlabel=&quot;x&quot;, ylabel=&quot;y&quot;, legend=false)</code></pre><p><img src="../2889699143.png" alt/></p><p>We split the observations into train and test data.</p><pre><code class="language-julia hljs">x_train = x[1:8]
y_train = y[1:8]
x_test = x[9:end]
y_test = y[9:end]</code></pre><p>We instantiate a Gaussian process with a Matern kernel. The kernel has fixed variance and length scale parameters of default value 1.</p><pre><code class="language-julia hljs">f = GP(Matern52Kernel())</code></pre><p>We create a finite dimentional projection at the inputs of the training dataset observed under Gaussian noise with standard deviation <span>$\sigma = 0.1$</span>, and compute the log-likelihood of the outputs of the training dataset.</p><pre><code class="language-julia hljs">fx = f(x_train, 0.1)
logpdf(fx, y_train)</code></pre><pre><code class="nohighlight hljs">-25.53057444906227</code></pre><p>We compute the posterior Gaussian process given the training data, and calculate the log-likelihood of the test dataset.</p><pre><code class="language-julia hljs">p_fx = posterior(fx, y_train)
logpdf(p_fx(x_test), y_test)</code></pre><pre><code class="nohighlight hljs">-232.51565975751606</code></pre><p>We plot the posterior Gaussian process along with the observations.</p><pre><code class="language-julia hljs">scatter(
    x_train,
    y_train;
    xlim=(0, 1),
    xlabel=&quot;x&quot;,
    ylabel=&quot;y&quot;,
    title=&quot;posterior (default parameters)&quot;,
    label=&quot;Train Data&quot;,
)
scatter!(x_test, y_test; label=&quot;Test Data&quot;)
plot!(0:0.001:1, p_fx; label=false)</code></pre><p><img src="../3599786597.png" alt/></p><h2 id="Markov-Chain-Monte-Carlo"><a class="docs-heading-anchor" href="#Markov-Chain-Monte-Carlo">Markov Chain Monte Carlo</a><a id="Markov-Chain-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Markov-Chain-Monte-Carlo" title="Permalink"></a></h2><p>Previously we computed the log likelihood of the untuned kernel parameters of the GP. We now also perform approximate inference over said kernel parameters using different Markov chain Monte Carlo (MCMC) methods. I.e., we approximate the posterior distribution of the kernel parameters with samples from a Markov chain.</p><p>We define a function which returns the log-likelihood of the data for different variance and inverse lengthscale parameters of the Matern kernel. We ensure that these parameters are positive with the softplus function</p><p class="math-container">\[f(x) = \log (1 + \exp x).\]</p><pre><code class="language-julia hljs">function gp_loglikelihood(x, y)
    function loglikelihood(params)
        kernel =
            softplus(params[1]) * (Matern52Kernel() ∘ ScaleTransform(softplus(params[2])))
        f = GP(kernel)
        fx = f(x, 0.1)
        return logpdf(fx, y)
    end
    return loglikelihood
end

const loglik_train = gp_loglikelihood(x_train, y_train)</code></pre><p>We define a Gaussian prior for the joint distribution of the two transformed kernel parameters. We assume that both parameters are independent with mean 0 and variance 1.</p><pre><code class="language-julia hljs">logprior(params) = logpdf(MvNormal(2, 1), params)</code></pre><h3 id="Hamiltonian-Monte-Carlo"><a class="docs-heading-anchor" href="#Hamiltonian-Monte-Carlo">Hamiltonian Monte Carlo</a><a id="Hamiltonian-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Hamiltonian-Monte-Carlo" title="Permalink"></a></h3><p>We start with a Hamiltonian Monte Carlo (HMC) sampler. More precisely, we use the <a href="http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf">No-U-Turn sampler (NUTS)</a>, which is provided by the Julia packages <a href="https://github.com/TuringLang/AdvancedHMC.jl/">AdvancedHMC.jl</a> and <a href="https://github.com/tpapp/DynamicHMC.jl/">DynamicHMC.jl</a>.</p><h4 id="AdvancedHMC"><a class="docs-heading-anchor" href="#AdvancedHMC">AdvancedHMC</a><a id="AdvancedHMC-1"></a><a class="docs-heading-anchor-permalink" href="#AdvancedHMC" title="Permalink"></a></h4><p>We start with performing inference with AdvancedHMC.</p><pre><code class="language-julia hljs">using AdvancedHMC
using ForwardDiff</code></pre><p>Set the number of samples to draw and warmup iterations.</p><pre><code class="language-julia hljs">n_samples = 2_000
n_adapts = 1_000</code></pre><p>Define a Hamiltonian system of the log joint probability.</p><pre><code class="language-julia hljs">logjoint_train(params) = loglik_train(params) + logprior(params)
metric = DiagEuclideanMetric(2)
hamiltonian = Hamiltonian(metric, logjoint_train, ForwardDiff)</code></pre><p>Define a leapfrog solver, with initial step size chosen heuristically.</p><pre><code class="language-julia hljs">initial_params = rand(2)
initial_ϵ = find_good_stepsize(hamiltonian, initial_params)
integrator = Leapfrog(initial_ϵ)</code></pre><p>Define an HMC sampler, with the following components:</p><ul><li>multinomial sampling scheme,</li><li>generalised No-U-Turn criteria, and</li><li>windowed adaption for step-size and diagonal mass matrix</li></ul><pre><code class="language-julia hljs">proposal = NUTS{MultinomialTS,GeneralisedNoUTurn}(integrator)
adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))</code></pre><p>We draw samples from the posterior distribution of kernel parameters. These samples are in the unconstrained space <span>$\mathbb{R}^2$</span>.</p><pre><code class="language-julia hljs">samples, _ = sample(
    hamiltonian, proposal, initial_params, n_samples, adaptor, n_adapts; progress=false
)</code></pre><pre><code class="nohighlight hljs">┌ Info: Finished 1000 adapation steps
│   adaptor =
│    StanHMCAdaptor(
│        pc=WelfordVar,
│        ssa=NesterovDualAveraging(γ=0.05, t_0=10.0, κ=0.75, δ=0.8, state.ϵ=0.8632215254591531),
│        init_buffer=75, term_buffer=50, window_size=25,
│        state=window(76, 950), window_splits(100, 150, 250, 450, 950)
│    )
│   τ.integrator = Leapfrog(ϵ=0.863)
└   h.metric = DiagEuclideanMetric([0.4205557129728238, 0.4163 ...])
┌ Info: Finished 2000 sampling steps for 1 chains in 1.968339499 (s)
│   h = Hamiltonian(metric=DiagEuclideanMetric([0.4205557129728238, 0.4163 ...]))
│   τ = NUTS{MultinomialTS,Generalised}(integrator=Leapfrog(ϵ=0.863), max_depth=10), Δ_max=1000.0)
│   EBFMI_est = 1.143201310054058
└   average_acceptance_rate = 0.8561286461510924
</code></pre><p>We transform the samples back to the constrained space and compute the mean of both parameters:</p><pre><code class="language-julia hljs">samples_constrained = [map(softplus, p) for p in samples]
mean_samples = mean(samples_constrained)</code></pre><pre><code class="nohighlight hljs">2-element Vector{Float64}:
 2.3256789662018496
 2.264465149411044</code></pre><p>We plot a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.</p><pre><code class="language-julia hljs">histogram(
    reduce(hcat, samples_constrained)&#39;;
    xlabel=&quot;sample&quot;,
    ylabel=&quot;counts&quot;,
    layout=2,
    title=[&quot;variance&quot; &quot;inverse length scale&quot;],
    legend=false,
)
vline!(mean_samples&#39;; linewidth=2)</code></pre><p><img src="../2665854652.png" alt/></p><p>We approximate the log-likelihood of the test data using the posterior Gaussian processes for kernels with the sampled kernel parameters. We can observe that there is a significant improvement over the log-likelihood of the test data with respect to the posterior Gaussian process with default kernel parameters of value 1.</p><pre><code class="language-julia hljs">function gp_posterior(x, y, p)
    kernel = softplus(p[1]) * (Matern52Kernel() ∘ ScaleTransform(softplus(p[2])))
    f = GP(kernel)
    return posterior(f(x, 0.1), y)
end

mean(logpdf(gp_posterior(x_train, y_train, p)(x_test), y_test) for p in samples)</code></pre><pre><code class="nohighlight hljs">-7.681537641632703</code></pre><p>We sample a function from the posterior GP for the final 100 samples of kernel parameters.</p><pre><code class="language-julia hljs">plt = scatter(
    x_train,
    y_train;
    xlim=(0, 1),
    xlabel=&quot;x&quot;,
    ylabel=&quot;y&quot;,
    title=&quot;posterior (AdvancedHMC)&quot;,
    label=&quot;Train Data&quot;,
)
scatter!(plt, x_test, y_test; label=&quot;Test Data&quot;)
for p in samples[(end - 100):end]
    sampleplot!(plt, 0:0.02:1, gp_posterior(x_train, y_train, p))
end
plt</code></pre><p><img src="../2956197709.png" alt/></p><h4 id="DynamicHMC"><a class="docs-heading-anchor" href="#DynamicHMC">DynamicHMC</a><a id="DynamicHMC-1"></a><a class="docs-heading-anchor-permalink" href="#DynamicHMC" title="Permalink"></a></h4><p>We repeat the inference with DynamicHMC. DynamicHMC requires us to implement the LogDensityProblems interface for <code>loglik_train</code>.</p><pre><code class="language-julia hljs">using DynamicHMC
using LogDensityProblems

# Log joint density
function LogDensityProblems.logdensity(ℓ::typeof(loglik_train), params)
    return ℓ(params) + logprior(params)
end

# The parameter space is two-dimensional
LogDensityProblems.dimension(::typeof(loglik_train)) = 2

# `loglik_train` does not allow to evaluate derivatives of
# the log-likelihood function
function LogDensityProblems.capabilities(::Type{&lt;:typeof(loglik_train)})
    return LogDensityProblems.LogDensityOrder{0}()
end</code></pre><p>Now we can draw samples from the posterior distribution of kernel parameters with DynamicHMC. Again we use <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a> to compute the derivatives of the log joint density with automatic differentiation.</p><pre><code class="language-julia hljs">samples =
    mcmc_with_warmup(
        Random.GLOBAL_RNG,
        ADgradient(:ForwardDiff, loglik_train),
        n_samples;
        reporter=NoProgressReport(),
    ).chain</code></pre><p>We transform the samples back to the constrained space and compute the mean of both parameters:</p><pre><code class="language-julia hljs">samples_constrained = [map(softplus, p) for p in samples]
mean_samples = mean(samples_constrained)</code></pre><pre><code class="nohighlight hljs">2-element Vector{Float64}:
 2.3213100504593784
 2.2796598526163394</code></pre><p>We plot a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.</p><pre><code class="language-julia hljs">histogram(
    reduce(hcat, samples_constrained)&#39;;
    xlabel=&quot;sample&quot;,
    ylabel=&quot;counts&quot;,
    layout=2,
    title=[&quot;variance&quot; &quot;inverse length scale&quot;],
    legend=false,
)
vline!(mean_samples&#39;; linewidth=2)</code></pre><p><img src="../2665854652.png" alt/></p><p>Again we can observe that there is a significant improvement over the log-likelihood of the test data with respect to the posterior Gaussian process with default kernel parameters.</p><pre><code class="language-julia hljs">mean(logpdf(gp_posterior(x_train, y_train, p)(x_test), y_test) for p in samples)</code></pre><pre><code class="nohighlight hljs">-8.31421172398695</code></pre><p>We sample a function from the posterior GP for the final 100 samples of kernel parameters.</p><pre><code class="language-julia hljs">plt = scatter(
    x_train,
    y_train;
    xlim=(0, 1),
    xlabel=&quot;x&quot;,
    ylabel=&quot;y&quot;,
    title=&quot;posterior (DynamicHMC)&quot;,
    label=&quot;Train Data&quot;,
)
scatter!(plt, x_test, y_test; label=&quot;Test Data&quot;)
for p in samples[(end - 100):end]
    sampleplot!(plt, 0:0.02:1, gp_posterior(x_train, y_train, p))
end
plt</code></pre><p><img src="../3316964398.png" alt/></p><h3 id="Elliptical-slice-sampling"><a class="docs-heading-anchor" href="#Elliptical-slice-sampling">Elliptical slice sampling</a><a id="Elliptical-slice-sampling-1"></a><a class="docs-heading-anchor-permalink" href="#Elliptical-slice-sampling" title="Permalink"></a></h3><p>Instead of HMC, we use <a href="http://proceedings.mlr.press/v9/murray10a/murray10a.pdf">elliptical slice sampling</a> which is provided by the Julia package <a href="https://github.com/TuringLang/EllipticalSliceSampling.jl/">EllipticalSliceSampling.jl</a>.</p><pre><code class="language-julia hljs">using EllipticalSliceSampling</code></pre><p>We draw 2000 samples from the posterior distribution of kernel parameters.</p><pre><code class="language-julia hljs">samples = sample(ESSModel(
    MvNormal(2, 1), # Gaussian prior
    loglik_train,
), ESS(), n_samples; progress=false)</code></pre><p>We transform the samples back to the constrained space and compute the mean of both parameters:</p><pre><code class="language-julia hljs">samples_constrained = [map(softplus, p) for p in samples]
mean_samples = mean(samples_constrained)</code></pre><pre><code class="nohighlight hljs">2-element Vector{Float64}:
 2.300877326070935
 2.176876054233843</code></pre><p>We plot a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.</p><pre><code class="language-julia hljs">histogram(
    reduce(hcat, samples_constrained)&#39;;
    xlabel=&quot;sample&quot;,
    ylabel=&quot;counts&quot;,
    layout=2,
    title=[&quot;variance&quot; &quot;inverse length scale&quot;],
)
vline!(mean_samples&#39;; layout=2, labels=&quot;mean&quot;)</code></pre><p><img src="../1312685123.png" alt/></p><p>Again we can observe that there is a significant improvement over the log-likelihood of the test data with respect to the posterior Gaussian process with default kernel parameters.</p><pre><code class="language-julia hljs">mean(logpdf(gp_posterior(x_train, y_train, p)(x_test), y_test) for p in samples)</code></pre><pre><code class="nohighlight hljs">-16.58193064236488</code></pre><p>We sample a function from the posterior GP for the final 100 samples of kernel parameters.</p><pre><code class="language-julia hljs">plt = scatter(
    x_train,
    y_train;
    xlim=(0, 1),
    xlabel=&quot;x&quot;,
    ylabel=&quot;y&quot;,
    title=&quot;posterior (EllipticalSliceSampling)&quot;,
    label=&quot;Train Data&quot;,
)
scatter!(plt, x_test, y_test; label=&quot;Test Data&quot;)
for p in samples[(end - 100):end]
    sampleplot!(plt, 0:0.02:1, gp_posterior(x_train, y_train, p))
end
plt</code></pre><p><img src="../4190158528.png" alt/></p><h2 id="Variational-Inference"><a class="docs-heading-anchor" href="#Variational-Inference">Variational Inference</a><a id="Variational-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Variational-Inference" title="Permalink"></a></h2><p>Sanity check for the Evidence Lower BOund (ELBO) implemented according to M. K. Titsias&#39;s <em>Variational learning of inducing variables in sparse Gaussian processes</em>.</p><pre><code class="language-julia hljs">elbo(fx, y_train, f(rand(5)))</code></pre><pre><code class="nohighlight hljs">-25.86916652578844</code></pre><p>We use the LBFGS algorithm to maximize the given ELBO. It is provided by the Julia package <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a>.</p><pre><code class="language-julia hljs">using Optim</code></pre><p>We define a function which returns the negative ELBO for different variance and inverse lengthscale parameters of the Matern kernel and different pseudo-points. We ensure that the kernel parameters are positive with the softplus function</p><p class="math-container">\[f(x) = \log (1 + \exp x),\]</p><p>and that the pseudo-points are in the unit interval <span>$[0,1]$</span> with the logistic function</p><p class="math-container">\[f(x) = \frac{1}{1 + \exp{(-x)}}.\]</p><pre><code class="language-julia hljs">function objective_function(x, y)
    function negative_elbo(params)
        kernel =
            softplus(params[1]) * (Matern52Kernel() ∘ ScaleTransform(softplus(params[2])))
        f = GP(kernel)
        fx = f(x, 0.1)
        z = logistic.(params[3:end])
        fz = f(z, 1e-6)  # &quot;observing&quot; the latent process with some (small) amount of jitter improves numerical stability
        return -elbo(fx, y, fz)
    end
    return negative_elbo
end</code></pre><p>We randomly initialize the kernel parameters and 5 pseudo points, and minimize the negative ELBO with the LBFGS algorithm and obtain the following optimal parameters:</p><pre><code class="language-julia hljs">x0 = rand(7)
opt = optimize(objective_function(x_train, y_train), x0, LBFGS())</code></pre><pre><code class="nohighlight hljs"> * Status: success

 * Candidate solution
    Final objective value:     1.086925e+01

 * Found with
    Algorithm:     L-BFGS

 * Convergence measures
    |x - x&#39;|               = 2.11e-09 ≰ 0.0e+00
    |x - x&#39;|/|x&#39;|          = 2.52e-10 ≰ 0.0e+00
    |f(x) - f(x&#39;)|         = 0.00e+00 ≤ 0.0e+00
    |f(x) - f(x&#39;)|/|f(x&#39;)| = 0.00e+00 ≤ 0.0e+00
    |g(x)|                 = 9.39e-09 ≤ 1.0e-08

 * Work counters
    Seconds run:   1  (vs limit Inf)
    Iterations:    53
    f(x) calls:    138
    ∇f(x) calls:   138
</code></pre><pre><code class="language-julia hljs">opt.minimizer</code></pre><pre><code class="nohighlight hljs">7-element Vector{Float64}:
  8.379380144211812
  3.9327375477933506
  1.8479571191117026
  0.691710280237785
  1.2763097379941533
 -1.758326768058792
 -4.133679972893883</code></pre><p>The optimized value of the variance is</p><pre><code class="language-julia hljs">softplus(opt.minimizer[1])</code></pre><pre><code class="nohighlight hljs">8.379609670059358</code></pre><p>and of the inverse lengthscale is</p><pre><code class="language-julia hljs">softplus(opt.minimizer[2])</code></pre><pre><code class="nohighlight hljs">3.952138104662189</code></pre><p>We compute the log-likelihood of the test data for the resulting approximate posterior. We can observe that there is a significant improvement over the log-likelihood with the default kernel parameters of value 1.</p><pre><code class="language-julia hljs">opt_kernel =
    softplus(opt.minimizer[1]) *
    (Matern52Kernel() ∘ ScaleTransform(softplus(opt.minimizer[2])))
opt_f = GP(opt_kernel)
opt_fx = opt_f(x_train, 0.1)
ap = approx_posterior(VFE(), opt_fx, y_train, opt_f(logistic.(opt.minimizer[3:end])))
logpdf(ap(x_test), y_test)</code></pre><pre><code class="nohighlight hljs">-1.0522026774312465</code></pre><p>We visualize the approximate posterior with optimized parameters.</p><pre><code class="language-julia hljs">scatter(
    x_train,
    y_train;
    xlim=(0, 1),
    xlabel=&quot;x&quot;,
    ylabel=&quot;y&quot;,
    title=&quot;posterior (VI with sparse grid)&quot;,
    label=&quot;Train Data&quot;,
)
scatter!(x_test, y_test; label=&quot;Test Data&quot;)
plot!(0:0.001:1, ap; label=false)
vline!(logistic.(opt.minimizer[3:end]); label=&quot;Pseudo-points&quot;)</code></pre><p><img src="../2191452572.png" alt/></p><h2 id="Exact-Gaussian-Process-Inference"><a class="docs-heading-anchor" href="#Exact-Gaussian-Process-Inference">Exact Gaussian Process Inference</a><a id="Exact-Gaussian-Process-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Exact-Gaussian-Process-Inference" title="Permalink"></a></h2><p>Here we use Type-II MLE to train the hyperparameters of the Gaussian process. This means that our loss function is the negative log marginal likelihood.</p><p>We re-calculate the log-likelihood of the test dataset with the default kernel parameters of value 1 for the sake of comparison.</p><pre><code class="language-julia hljs">logpdf(p_fx(x_test), y_test)</code></pre><pre><code class="nohighlight hljs">-232.51565975751606</code></pre><p>We define a function which returns the negative log marginal likelihood for different variance and inverse lengthscale parameters of the Matern kernel and different pseudo-points. We ensure that the kernel parameters are positive with the softplus function <span>$f(x) = \log (1 + \exp x)$</span>.</p><pre><code class="language-julia hljs">function loss_function(x, y)
    function negativelogmarginallikelihood(params)
        kernel =
            softplus(params[1]) * (Matern52Kernel() ∘ ScaleTransform(softplus(params[2])))
        f = GP(kernel)
        fx = f(x, 0.1)
        return -logpdf(fx, y)
    end
    return negativelogmarginallikelihood
end
</code></pre><p>We randomly initialize the kernel parameters, and minimize the negative log marginal likelihood with the LBFGS algorithm and obtain the following optimal parameters:</p><pre><code class="language-julia hljs">θ0 = randn(2)
opt = Optim.optimize(loss_function(x_train, y_train), θ0, LBFGS())</code></pre><pre><code class="nohighlight hljs"> * Status: success

 * Candidate solution
    Final objective value:     1.085252e+01

 * Found with
    Algorithm:     L-BFGS

 * Convergence measures
    |x - x&#39;|               = 1.50e-05 ≰ 0.0e+00
    |x - x&#39;|/|x&#39;|          = 1.79e-06 ≰ 0.0e+00
    |f(x) - f(x&#39;)|         = 3.91e-12 ≰ 0.0e+00
    |f(x) - f(x&#39;)|/|f(x&#39;)| = 3.60e-13 ≰ 0.0e+00
    |g(x)|                 = 7.02e-10 ≤ 1.0e-08

 * Work counters
    Seconds run:   0  (vs limit Inf)
    Iterations:    13
    f(x) calls:    51
    ∇f(x) calls:   51
</code></pre><pre><code class="language-julia hljs">opt.minimizer</code></pre><pre><code class="nohighlight hljs">2-element Vector{Float64}:
 8.385520557281978
 3.9687942115144725</code></pre><p>The optimized value of the variance is</p><pre><code class="language-julia hljs">softplus(opt.minimizer[1])</code></pre><pre><code class="nohighlight hljs">8.385748678224516</code></pre><p>and of the inverse lengthscale is</p><pre><code class="language-julia hljs">softplus(opt.minimizer[2])</code></pre><pre><code class="nohighlight hljs">3.9875141001442764</code></pre><p>We compute the log-likelihood of the test data for the resulting optimized posterior. We can observe that there is a significant improvement over the log-likelihood with the default kernel parameters of value 1.</p><pre><code class="language-julia hljs">opt_kernel =
    softplus(opt.minimizer[1]) *
    (Matern52Kernel() ∘ ScaleTransform(softplus(opt.minimizer[2])))

opt_f = GP(opt_kernel)
opt_fx = opt_f(x_train, 0.1)
opt_p_fx = posterior(opt_fx, y_train)
logpdf(opt_p_fx(x_test), y_test)</code></pre><pre><code class="nohighlight hljs">-1.084976751507031</code></pre><p>We visualize the posterior with optimized parameters.</p><pre><code class="language-julia hljs">scatter(
    x_train,
    y_train;
    xlim=(0, 1),
    xlabel=&quot;x&quot;,
    ylabel=&quot;y&quot;,
    title=&quot;posterior (optimized parameters)&quot;,
    label=&quot;Train Data&quot;,
)
scatter!(x_test, y_test; label=&quot;Test Data&quot;)
plot!(0:0.001:1, opt_p_fx; label=false)</code></pre><p><img src="../346125811.png" alt/></p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../concrete_features/">« Concrete Features</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.5 on <span class="colophon-date" title="Sunday 15 August 2021 18:28">Sunday 15 August 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>

