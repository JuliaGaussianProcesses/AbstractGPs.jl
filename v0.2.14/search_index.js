var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [AbstractGPs]","category":"page"},{"location":"api/#AbstractGPs.AbstractGP","page":"API","title":"AbstractGPs.AbstractGP","text":"abstract type AbstractGP end\n\nSupertype for various Gaussian process (GP) types. A common interface is provided for interacting with each of these objects. See [1] for an overview of GPs.\n\n[1] - C. E. Rasmussen and C. Williams. \"Gaussian processes for machine learning\".  MIT Press. 2006.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.ConstMean","page":"API","title":"AbstractGPs.ConstMean","text":"ConstMean{T<:Real} <: MeanFunction\n\nReturns c everywhere.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.CustomMean","page":"API","title":"AbstractGPs.CustomMean","text":"CustomMean{Tf} <: MeanFunction\n\nA wrapper around whatever unary function you fancy. Must be able to be mapped over an AbstractVector of inputs.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.FiniteGP","page":"API","title":"AbstractGPs.FiniteGP","text":"FiniteGP{Tf<:AbstractGP, Tx<:AbstractVector, TΣy}\n\nThe finite-dimensional projection of the AbstractGP f at x. Assumed to be observed under Gaussian noise with zero mean and covariance matrix Σ\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.GP","page":"API","title":"AbstractGPs.GP","text":"GP{Tm<:MeanFunction, Tk<:Kernel}\n\nA Gaussian Process (GP) with known mean and kernel. See e.g. [1] for an introduction.\n\nZero Mean\n\nIf only one argument is provided, assume the mean to be zero everywhere:\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(5);\n\njulia> mean(f(x)) == zeros(5)\ntrue\n\njulia> cov(f(x)) == kernelmatrix(Matern32Kernel(), x)\ntrue\n\nConstant Mean\n\nIf a Real is provided as the first argument, assume the mean function is constant with that value\n\njulia> f = GP(5.0, Matern32Kernel());\n\njulia> x = randn(5);\n\njulia> mean(f(x)) == 5.0 .* ones(5)\ntrue\n\njulia> cov(f(x)) == kernelmatrix(Matern32Kernel(), x)\ntrue\n\nCustom Mean\n\nProvide an arbitrary function to compute the mean:\n\njulia> f = GP(x -> sin(x) + cos(x / 2), Matern32Kernel());\n\njulia> x = randn(5);\n\njulia> mean(f(x)) == sin.(x) .+ cos.(x ./ 2)\ntrue\n\njulia> cov(f(x)) == kernelmatrix(Matern32Kernel(), x)\ntrue\n\n[1] - C. E. Rasmussen and C. Williams. \"Gaussian processes for machine learning\".  MIT Press. 2006.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.LatentFiniteGP","page":"API","title":"AbstractGPs.LatentFiniteGP","text":"LatentFiniteGP(fx<:FiniteGP, lik)\n\nfx is a FiniteGP.\nlik is the log likelihood function which maps sample from f to corresposing \n\nconditional likelihood distributions.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.LatentGP","page":"API","title":"AbstractGPs.LatentGP","text":"LatentGP(f<:GP, lik, Σy)\n\nf is a AbstractGP.\nlik is the log likelihood function which maps sample from f to corresposing \n\nconditional likelihood distributions.\n\nΣy is the observation noise\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.ZeroMean","page":"API","title":"AbstractGPs.ZeroMean","text":"ZeroMean{T<:Real} <: MeanFunction\n\nReturns zero(T) everywhere.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.approx_posterior-Tuple{VFE,AbstractGPs.FiniteGP,AbstractArray{var\"#s16\",1} where var\"#s16\"<:Real,AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.approx_posterior","text":"approx_posterior(::VFE, fx::FiniteGP, y::AbstractVector{<:Real}, u::FiniteGP)\n\nCompute the optimal approximate posterior [1] over the process f, given observations y of f at x, and inducing points u, where u = f(z) for some inducing inputs z.\n\n[1] - M. K. Titsias. \"Variational learning of inducing variables in sparse Gaussian processes\". In: Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics. 2009.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.cov_diag-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}","page":"API","title":"AbstractGPs.cov_diag","text":"cov_diag(f::AbstractGP, x::AbstractVector)\n\nCompute only the diagonal elements of cov(f(x)).\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.dtc-Tuple{AbstractGPs.FiniteGP,AbstractArray{var\"#s15\",1} where var\"#s15\"<:Real,AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.dtc","text":"dtc(f::FiniteGP, y::AbstractVector{<:Real}, u::FiniteGP)\n\nThe Deterministic Training Conditional (DTC) [1]. y are observations of f, and u are pseudo-points.\n\njulia> f = GP(Matern52Kernel());\n\njulia> x = randn(1000);\n\njulia> z = range(-5.0, 5.0; length=256);\n\njulia> y = rand(f(x, 0.1));\n\njulia> isapprox(dtc(f(x, 0.1), y, f(z)), logpdf(f(x, 0.1), y); atol=1e-3, rtol=1e-3)\ntrue\n\n[1] - M. Seeger, C. K. I. Williams and N. D. Lawrence. \"Fast Forward Selection to Speed Up Sparse Gaussian Process Regression\". In: Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics. 2003\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.elbo-Tuple{AbstractGPs.FiniteGP,AbstractArray{var\"#s13\",1} where var\"#s13\"<:Real,AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.elbo","text":"elbo(f::FiniteGP, y::AbstractVector{<:Real}, u::FiniteGP)\n\nThe Titsias Evidence Lower BOund (ELBO) [1]. y are observations of f, and u are pseudo-points, where u = f(z) for some z.\n\njulia> f = GP(Matern52Kernel());\n\njulia> x = randn(1000);\n\njulia> z = range(-5.0, 5.0; length=13);\n\njulia> y = rand(f(x, 0.1));\n\njulia> elbo(f(x, 0.1), y, f(z)) < logpdf(f(x, 0.1), y)\ntrue\n\n[1] - M. K. Titsias. \"Variational learning of inducing variables in sparse Gaussian processes\". In: Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics. 2009.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.marginals-Tuple{AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.marginals","text":"marginals(f::FiniteGP)\n\nCompute a vector of Normal distributions representing the marginals of f efficiently. In particular, the off-diagonal elements of cov(f(x)) are never computed.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(11);\n\njulia> fs = marginals(f(x));\n\njulia> mean.(fs) == mean(f(x))\ntrue\n\njulia> std.(fs) == sqrt.(diag(cov(f(x))))\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.mean_and_cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}","page":"API","title":"AbstractGPs.mean_and_cov","text":"mean_and_cov(f::AbstractGP, x::AbstractVector)\n\nCompute both mean(f(x)) and cov(f(x)). Sometimes more efficient than separately computation, particularly for posteriors.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.mean_and_cov-Tuple{AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.mean_and_cov","text":"mean_and_cov(f::FiniteGP)\n\nEquivalent to (mean(f), cov(f)), but sometimes more efficient to compute them jointly than separately.\n\njulia> fx = GP(SqExponentialKernel())(range(-3.0, 3.0; length=10), 0.1);\n\njulia> mean_and_cov(fx) == (mean(fx), cov(fx))\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.mean_and_cov_diag-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}","page":"API","title":"AbstractGPs.mean_and_cov_diag","text":"mean_and_cov_diag(f::AbstractGP, x::AbstractVector)\n\nCompute both mean(f(x)) and the diagonal elements of cov(f(x)). Sometimes more efficient than separately computation, particularly for posteriors.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.posterior-Tuple{AbstractGPs.FiniteGP,AbstractArray{var\"#s13\",1} where var\"#s13\"<:Real}","page":"API","title":"AbstractGPs.posterior","text":"posterior(fx::FiniteGP, y::AbstractVector{<:Real})\n\nConstructs the posterior distribution over fx.f given observations y at x made under noise fx.Σy. This is another AbstractGP object. See chapter 2 of [1] for a recap on exact inference in GPs. This posterior process has mean function\n\nm_posterior(x) = m(x) + k(x, fx.x) inv(cov(fx)) (y - mean(fx))\n\nand kernel\n\nk_posterior(x, z) = k(x, z) - k(x, fx.x) inv(cov(fx)) k(fx.x, z)\n\nwhere m and k are the mean function and kernel of fx.f respectively.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.posterior-Tuple{AbstractGPs.FiniteGP{var\"#s13\",Tx,TΣ} where TΣ where Tx<:(AbstractArray{T,1} where T) where var\"#s13\"<:AbstractGPs.PosteriorGP,AbstractArray{var\"#s23\",1} where var\"#s23\"<:Real}","page":"API","title":"AbstractGPs.posterior","text":"posterior(fx::FiniteGP{<:PosteriorGP}, y::AbstractVector{<:Real})\n\nConstructs the posterior distribution over fx.f when f is itself a PosteriorGP by updating the cholesky factorisation of the covariance matrix and avoiding recomputing it from original covariance matrix. It does this by using update_chol functionality.\n\nOther aspects are similar to a regular posterior.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.sampleplot!-Tuple","page":"API","title":"AbstractGPs.sampleplot!","text":"sampleplot(GP::FiniteGP, samples)\n\nPlot samples from the given FiniteGP. Make sure to run using Plots before using this  function. \n\nExample\n\nusing Plots\nf = GP(SqExponentialKernel())\nsampleplot(f(rand(10)), 10; markersize=5)\n\nThe given example plots 10 samples from the given FiniteGP. The markersize is modified from default of 0.5 to 5.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.sampleplot!-Tuple{RecipesBase.AbstractPlot,Vararg{Any,N} where N}","page":"API","title":"AbstractGPs.sampleplot!","text":"sampleplot(GP::FiniteGP, samples)\n\nPlot samples from the given FiniteGP. Make sure to run using Plots before using this  function. \n\nExample\n\nusing Plots\nf = GP(SqExponentialKernel())\nsampleplot(f(rand(10)), 10; markersize=5)\n\nThe given example plots 10 samples from the given FiniteGP. The markersize is modified from default of 0.5 to 5.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.sampleplot-Tuple","page":"API","title":"AbstractGPs.sampleplot","text":"sampleplot(GP::FiniteGP, samples)\n\nPlot samples from the given FiniteGP. Make sure to run using Plots before using this  function. \n\nExample\n\nusing Plots\nf = GP(SqExponentialKernel())\nsampleplot(f(rand(10)), 10; markersize=5)\n\nThe given example plots 10 samples from the given FiniteGP. The markersize is modified from default of 0.5 to 5.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.update_approx_posterior-Tuple{AbstractGPs.ApproxPosteriorGP,AbstractGPs.FiniteGP,AbstractArray{var\"#s15\",1} where var\"#s15\"<:Real}","page":"API","title":"AbstractGPs.update_approx_posterior","text":"function update_approx_posterior(\n    f_post_approx::ApproxPosteriorGP,\n    fx::FiniteGP,\n    y::AbstractVector{<:Real}\n)\n\nUpdate the ApproxPosteriorGP given a new set of observations. Here, we retain the same  of pseudo-points.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.update_approx_posterior-Tuple{AbstractGPs.ApproxPosteriorGP,AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.update_approx_posterior","text":"function update_approx_posterior(\n    f_post_approx::ApproxPosteriorGP,\n    u::FiniteGP,\n)\n\nUpdate the ApproxPosteriorGP given a new set of pseudo-points to append to the existing  set of pseudo points. \n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.update_chol-Tuple{LinearAlgebra.Cholesky,AbstractArray{T,2} where T,AbstractArray{T,2} where T}","page":"API","title":"AbstractGPs.update_chol","text":" update_chol(chol::Cholesky, C12::AbstractMatrix, C22::AbstractMatrix)\n\nLet C be the positive definite matrix comprising blocks\n\nC = [C11 C12;\n     C21 C22]\n\nwith upper-triangular cholesky factorisation comprising blocks\n\nU = [U11 U12;\n     0   U22]\n\nwhere U11 and U22 are themselves upper-triangular, and U11 = cholesky(C11).U. update_chol computes the updated Cholesky given original chol, C12, and C22.\n\nArguments\n\n - chol::Cholesky: The original cholesky decomposition\n - C12::AbstractMatrix: matrix of size (size(chol.U, 1), size(C22, 1))\n - C22::AbstractMatrix: positive-definite matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG,AbstractGPs.FiniteGP,Int64}","page":"API","title":"Base.rand","text":"rand(rng::AbstractRNG, f::FiniteGP, N::Int=1)\n\nObtain N independent samples from the marginals f using rng. Single-sample methods produce a length(f) vector. Multi-sample methods produce a length(f) x N Matrix.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(11);\n\njulia> rand(f(x)) isa Vector{Float64}\ntrue\n\njulia> rand(MersenneTwister(123456), f(x)) isa Vector{Float64}\ntrue\n\njulia> rand(f(x), 3) isa Matrix{Float64}\ntrue\n\njulia> rand(MersenneTwister(123456), f(x), 3) isa Matrix{Float64}\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.logpdf-Tuple{AbstractGPs.FiniteGP,Union{AbstractArray{var\"#s16\",1}, AbstractArray{var\"#s16\",2}} where var\"#s16\"<:Real}","page":"API","title":"Distributions.logpdf","text":"logpdf(f::FiniteGP, y::AbstractVecOrMat{<:Real})\n\nThe logpdf of y under f if is y isa AbstractVector. logpdf of each column of y if y isa Matrix.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(11);\n\njulia> y = rand(f(x));\n\njulia> logpdf(f(x), y) isa Real\ntrue\n\njulia> Y = rand(f(x), 3);\n\njulia> logpdf(f(x), Y) isa AbstractVector{<:Real}\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.logpdf-Tuple{AbstractGPs.LatentFiniteGP,NamedTuple{(:f, :y),T} where T<:Tuple}","page":"API","title":"Distributions.logpdf","text":"logpdf(lfgp::LatentFiniteGP, y::NamedTuple{(:f, :y)})\n\n    log p(y f x)\n\nReturns the joint log density of the gaussian process output f and real output y.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T,AbstractArray{T,1} where T}","page":"API","title":"Statistics.cov","text":"cov(f::AbstractGP, x::AbstractVector, y::AbstractVector)\n\nCompute the length(x) by length(y) cross-covariance matrix between f(x) and f(y).\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}","page":"API","title":"Statistics.cov","text":"cov(f::AbstractGP, x::AbstractVector)\n\nCompute the length(x) by length(x) covariance matrix of the multivariate Normal f(x).\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.FiniteGP,AbstractGPs.FiniteGP}","page":"API","title":"Statistics.cov","text":"cov(fx::FiniteGP, gx::FiniteGP)\n\nCompute the cross-covariance matrix between fx and gx.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x1 = randn(11);\n\njulia> x2 = randn(13);\n\njulia> cov(f(x1), f(x2)) == kernelmatrix(Matern32Kernel(), x1, x2)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.FiniteGP}","page":"API","title":"Statistics.cov","text":"cov(f::FiniteGP)\n\nCompute the covariance matrix of fx.\n\nNoise-free observations\n\njulia> f = GP(Matern52Kernel());\n\njulia> x = randn(11);\n\njulia> cov(f(x)) == kernelmatrix(Matern52Kernel(), x)\ntrue\n\nIsotropic observation noise\n\njulia> cov(f(x, 0.1)) == kernelmatrix(Matern52Kernel(), x) + 0.1 * I\ntrue\n\nIndependent anisotropic observation noise\n\njulia> s = rand(11);\n\njulia> cov(f(x, s)) == kernelmatrix(Matern52Kernel(), x) + Diagonal(s)\ntrue\n\nCorrelated observation noise\n\njulia> A = randn(11, 11); S = A'A;\n\njulia> cov(f(x, S)) == kernelmatrix(Matern52Kernel(), x) + S\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}","page":"API","title":"Statistics.mean","text":"mean(f::AbstractGP, x::AbstractVector)\n\nComputes the mean vector of the multivariate Normal f(x).\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{AbstractGPs.FiniteGP}","page":"API","title":"Statistics.mean","text":"mean(fx::FiniteGP)\n\nCompute the mean vector of fx.\n\njulia> f = GP(Matern52Kernel());\n\njulia> x = randn(11);\n\njulia> mean(f(x)) == zeros(11)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/master/examples/VariationalInference.jl\"","category":"page"},{"location":"examples/VariationalInference/#Approximate-Inference-on-Sparse-GPs-using-VI","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"","category":"section"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Loading the necessary packages and setting seed.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"using AbstractGPs, Plots, Random\nRandom.seed!(1234);\nnothing #hide","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Loading toy regression dataset taken from GPFlow examples.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"x = [0.8658165855998895, 0.6661700880180962, 0.8049218148148531, 0.7714303440386239,\n    0.14790478354654835, 0.8666105548197428, 0.007044577166530286, 0.026331737288148638,\n    0.17188596617099916, 0.8897812990554013, 0.24323574561119998, 0.028590102134105955];\ny = [1.5255314337144372, 3.6434202968230003, 3.010885733911661, 3.774442382979625,\n    3.3687639483798324, 1.5506452040608503, 3.790447985799683, 3.8689707574953,\n    3.4933565751758713, 1.4284538820635841, 3.8715350915692364, 3.7045949061144983];\nscatter(x, y, xlabel=\"x\", ylabel=\"y\")","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Split the observations into train and test set.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"(x_train, y_train) = (x[begin:8], y[begin:8]);\n(x_test, y_test) = (x[9:end], y[9:end]);\nnothing #hide","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Instantiate the kernel.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"k = Matern52Kernel()","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Instantiate a Gaussian Process with the given kernel k.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"f = GP(k);\nnothing #hide","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Instantiate a FiniteGP, a finite dimentional projection at the inputs of the dataset observed under Gaussian Noise with sigma = 0001 .","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"fx = f(x_train, 0.001);\nnothing #hide","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Calculating the exact posterior over f given y. The GP's kernel currently has some arbitrary fixed parameters.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"p_fx = posterior(fx, y_train)\n\nlogpdf(p_fx(x_test), y_test)","category":"page"},{"location":"examples/VariationalInference/#Variational-Inference","page":"Approximate Inference on Sparse GPs using VI","title":"Variational Inference","text":"","category":"section"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Sanity check for the Evidence Lower BOund (ELBO) implemented according to M. K. Titsias's Variational learning of inducing variables in sparse Gaussian processes","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"elbo(fx, y_train, f(rand(7)))","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"We will be using Optim.jl package's LBFGS algorithm to maximize the given ELBO.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"using Optim","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Create a helper function for optimization. It takes in the parameters (both variational and model parameters) and returns the negative of the ELBO.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"function optim_function(params; x=x_train, y=y_train)\n    kernel = ScaledKernel(\n        transform(\n            Matern52Kernel(),\n            ScaleTransform(exp.(params[1]))\n            ),\n        exp.(params[2])\n    )\n    f = GP(kernel)\n    fx = f(x, 0.1)\n    return -elbo(fx, y, f(params[3:end]))\nend","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Initialize the parameters (Varitational and Model parameters)","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"x0 = rand(7)","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Sanity check for the helper function. We intend to minimize the result of this function.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"-optim_function(x0)","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Optimize using Optim.jl package's LBFGS algorithm.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"opt = optimize(optim_function, x0, LBFGS())","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Optimal parameters:","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"opt.minimizer","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"ELBO with optimal parameters. We see that there is significant improvement when compared to the initial parameters.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"-optim_function(opt.minimizer; x=x_test, y=y_test)","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Visualize the posterior.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"opt_kernel = ScaledKernel(\n    transform(\n        Matern52Kernel(),\n        ScaleTransform(exp.(opt.minimizer[1]))\n        ),\n    exp.(opt.minimizer[2])\n)\nopt_f = GP(opt_kernel)\nopt_fx = opt_f(x_train, 0.1)\nap = approx_posterior(VFE(), opt_fx, y_train, opt_f(opt.minimizer[3:end]));\nnothing #hide","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"Average log-marginal-probability of data with posterior kernel parameter samples sampled using ESS. We can observe that there is significant improvement over exact posterior with default kernel parameters.","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"logpdf(ap(x_test), y_test)\n\nplt = plot(ap, 0:0.001:1, label=\"Approx Posterior\")\nplot!(plt, p_fx, 0:0.001:1, label=\"Exact Posterior\")\nscatter!(\n    plt,\n    opt.minimizer[3:end],\n    mean(rand(ap(opt.minimizer[3:end], 0.1), 100), dims=2),\n    label=\"Pseudo-points\"\n)\nscatter!(plt, x_train, y_train, label=\"Train Data\")\nscatter!(plt, x_test, y_test, label=\"Test Data\")\nplt","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"","category":"page"},{"location":"examples/VariationalInference/","page":"Approximate Inference on Sparse GPs using VI","title":"Approximate Inference on Sparse GPs using VI","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/master/examples/EllipticalSliceSampling.jl\"","category":"page"},{"location":"examples/EllipticalSliceSampling/#Approximate-Inference-using-ESS","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"","category":"section"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Loading the necessary packages and setting seed.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"using AbstractGPs, Plots, Random\nRandom.seed!(1234);\nnothing #hide","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Loading toy regression dataset taken from GPFlow examples.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"x = [0.8658165855998895, 0.6661700880180962, 0.8049218148148531, 0.7714303440386239,\n    0.14790478354654835, 0.8666105548197428, 0.007044577166530286, 0.026331737288148638,\n    0.17188596617099916, 0.8897812990554013, 0.24323574561119998, 0.028590102134105955];\ny = [1.5255314337144372, 3.6434202968230003, 3.010885733911661, 3.774442382979625,\n    3.3687639483798324, 1.5506452040608503, 3.790447985799683, 3.8689707574953,\n    3.4933565751758713, 1.4284538820635841, 3.8715350915692364, 3.7045949061144983];\nscatter(x, y, xlabel=\"x\", ylabel=\"y\")","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Split the observations into train and test set.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"(x_train, y_train) = (x[begin:8], y[begin:8]);\n(x_test, y_test) = (x[9:end], y[9:end]);\nnothing #hide","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Instantiating the kernel.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"k = Matern52Kernel()","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Instantiating a Gaussian Process with the given kernel k.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"f = GP(k)","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Instantiating a FiniteGP, a finite dimentional projection at the inputs of the dataset observed under Gaussian Noise with sigma = 0001 .","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"fx = f(x_train, 0.001)","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Data's log-likelihood w.r.t prior GP.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"logpdf(fx, y_train)","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Calculating the exact posterior over f given y. The GP's kernel currently has some arbitrary fixed parameters.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"p_fx = posterior(fx, y_train)","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Data's log-likelihood under the posterior GP. We see that it drastically increases.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"logpdf(p_fx(x_test), y_test)","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Plot the posterior p_fx along with the observations.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"plt = scatter(x_train, y_train; label = \"Train data\")\nscatter!(plt, x_test, y_test; label = \"Test data\")\nplot!(plt, p_fx, 0:0.001:1; label=\"Posterior\")","category":"page"},{"location":"examples/EllipticalSliceSampling/#Elliptical-Slice-Sampler","page":"Approximate Inference using ESS","title":"Elliptical Slice Sampler","text":"","category":"section"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Previously we computed the log likelihood of the untuned kernel parameters of the GP, -1285. We now also perform approximate inference over said kernel parameters using the Elliptical Slice Sampling provided by EllipticalSliceSampling.jl. We start of by loading necessary packages.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"using EllipticalSliceSampling, Distributions","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"We define a function which returns log-probability of the data under the GP / log-likelihood of the parameters of the GP.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"function logp(params; x=x_train, y=y_train)\n    kernel = ScaledKernel(\n        transform(\n            Matern52Kernel(),\n            ScaleTransform(exp(params[1]))\n        ),\n        exp(params[2])\n    )\n    f = GP(kernel)\n    fx = f(x, 0.1)\n    return logpdf(fx, y)\nend","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"We define a Gaussian prior over the joint distribution on kernel parameters space. Since we have only two parameters, we define a multi-variate Gaussian of dimension two.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"prior = MvNormal(2, 1)","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Sanity check for the defined logp function and prior distribution.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"logp(rand(prior))","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Generate 2,000 samples using ESS_mcmc provided by EllipticalSliceSampling.jl.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"samples = sample(ESSModel(prior, logp), ESS(), 2_000; progress=false)\nsamples_mat = reduce(hcat, samples)';\nnothing #hide","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Mean of samples of both the parameters.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"mean_params = mean(samples_mat, dims=1)","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Plot a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"plt = histogram(samples_mat; layout=2, labels=\"Param\")\nvline!(plt, mean_params; layout=2, label=\"Mean\")","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Average log-marginal-probability of data with posterior kernel parameter samples sampled using ESS. We can observe that there is significant improvement over exact posterior with default kernel parameters.","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"mean(logp(param; x=x_test, y=y_test) for param in samples)","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"Plot sampled functions from posterior with tuned parameters","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"plt = scatter(x_train, y_train; label=\"Train data\")\nscatter!(plt, x_test, y_test; label=\"Test data\")\nfor params in @view(samples[(end-100):end,:])\n    opt_kernel = ScaledKernel(\n        transform(\n            Matern52Kernel(),\n            ScaleTransform(exp(params[1]))\n        ),\n        exp(params[2])\n    )\n    f = GP(opt_kernel)\n    p_fx = posterior(f(x_train, 0.1), y_train)\n    sampleplot!(plt, p_fx(collect(0:0.02:1)), 1)\nend\nplt","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"","category":"page"},{"location":"examples/EllipticalSliceSampling/","page":"Approximate Inference using ESS","title":"Approximate Inference using ESS","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#AbstractGPs.jl","page":"Home","title":"AbstractGPs.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Abstract types and methods for Gaussian Processes.","category":"page"},{"location":"","page":"Home","title":"Home","text":"AbstractGPs.jl is a package that defines a low-level API for working with Gaussian processes (GPs), and basic functionality for working with them in the simplest cases. As such it is aimed more at developers and researchers who are interested in using it as a building block than end-users of GPs.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"#Setup","page":"Home","title":"Setup","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using AbstractGPs, Random\nrng = MersenneTwister(0)\n\n# Construct a zero-mean Gaussian process with a matern-3/2 kernel.\nf = GP(Matern32Kernel())\n\n# Specify some input and target locations.\nx = randn(rng, 10)\ny = randn(rng, 10)","category":"page"},{"location":"#Finite-dimensional-projection","page":"Home","title":"Finite dimensional projection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Look at the finite-dimensional projection of f at x, under zero-mean observation noise with variance 0.1.","category":"page"},{"location":"","page":"Home","title":"Home","text":"fx = f(x, 0.1)","category":"page"},{"location":"#Sample-from-GP-from-the-prior-at-x-under-noise.","page":"Home","title":"Sample from GP from the prior at x under noise.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"y_sampled = rand(rng, fx)","category":"page"},{"location":"#Compute-the-log-marginal-probability-of-y.","page":"Home","title":"Compute the log marginal probability of y.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"logpdf(fx, y)","category":"page"},{"location":"#Construct-the-posterior-process-implied-by-conditioning-f-at-x-on-y.","page":"Home","title":"Construct the posterior process implied by conditioning f at x on y.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"f_posterior = posterior(fx, y)","category":"page"},{"location":"#A-posterior-process-follows-the-AbstractGP-interface,-so-the-same-functions-which-work-on-the-posterior-as-on-the-prior.","page":"Home","title":"A posterior process follows the AbstractGP interface, so the same functions which work on the posterior as on the prior.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"rand(rng, f_posterior(x))\nlogpdf(f_posterior(x), y)","category":"page"},{"location":"#Compute-the-VFE-approximation-to-the-log-marginal-probability-of-y.","page":"Home","title":"Compute the VFE approximation to the log marginal probability of y.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here, z is a set of pseudo-points. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"z = randn(rng, 4)\nu = f(z)","category":"page"},{"location":"#Evidence-Lower-BOund-(ELBO)","page":"Home","title":"Evidence Lower BOund (ELBO)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We provide a ready implentation of elbo w.r.t to the pseudo points. We can perform Variational Inference on pseudo-points by maximizing the ELBO term w.r.t pseudo-points z and any kernel parameters. For more information, see examples. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"elbo(fx, y, u)","category":"page"},{"location":"#Construct-the-approximate-posterior-process-implied-by-the-VFE-approximation.","page":"Home","title":"Construct the approximate posterior process implied by the VFE approximation.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The optimal pseudo-points obtained above can be used to create a approximate/sparse posterior. This can be used like a regular posterior in many cases.","category":"page"},{"location":"","page":"Home","title":"Home","text":"f_approx_posterior = approx_posterior(VFE(), fx, y, u)","category":"page"},{"location":"#An-approximate-posterior-process-is-yet-another-AbstractGP,-so-you-can-do-things-with-it-like","page":"Home","title":"An approximate posterior process is yet another AbstractGP, so you can do things with it like","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"marginals(f_approx_posterior(x))","category":"page"},{"location":"#Sequential-Conditioning","page":"Home","title":"Sequential Conditioning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Sequential conditioning allows you to compute your posterior in an online fashion. We do this in an efficient manner by updating the cholesky factorisation of the covariance matrix and avoiding recomputing it from original covariance matrix.","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Define GP prior\nf = GP(SqExponentialKernel())","category":"page"},{"location":"#Exact-Posterior","page":"Home","title":"Exact Posterior","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"# Generate posterior with the first batch of data on the prior f1.\np_fx = posterior(f(x[1:3], 0.1), y[1:3])\n\n# Generate posterior with the second batch of data considering posterior p_fx1 as the prior.\np_p_fx = posterior(p_fx(x[4:10], 0.1), y[4:10])","category":"page"},{"location":"#Approximate-Posterior","page":"Home","title":"Approximate Posterior","text":"","category":"section"},{"location":"#Adding-observations-in-an-sequential-fashion","page":"Home","title":"Adding observations in an sequential fashion","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Z1 = rand(rng, 4)\nZ2 = rand(rng, 3)\np_fx = approx_posterior(VFE(), f(x[1:7], 0.1), y[1:7], f(Z))\nu_p_fx = update_approx_posterior(p_fx1, f(x[8:10], 0.1), y[8:10])","category":"page"},{"location":"#Adding-pseudo-points-in-an-sequential-fashion","page":"Home","title":"Adding pseudo-points in an sequential fashion","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"\np_fx1 = approx_posterior(VFE(), f(X, 0.1), y, f(Z1))\nu_p_fx1 = update_approx_posterior(p_fx1, f(Z2))","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/master/examples/HMC.jl\"","category":"page"},{"location":"examples/HMC/#Approximate-Inference-with-NUTS-HMC","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"","category":"section"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Loading the necessary packages and setting seed.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"using AbstractGPs, Plots, Random\nRandom.seed!(1234);\nnothing #hide","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Load toy regression dataset taken from GPFlow examples.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"x = [0.8658165855998895, 0.6661700880180962, 0.8049218148148531, 0.7714303440386239,\n    0.14790478354654835, 0.8666105548197428, 0.007044577166530286, 0.026331737288148638,\n    0.17188596617099916, 0.8897812990554013, 0.24323574561119998, 0.028590102134105955];\ny = [1.5255314337144372, 3.6434202968230003, 3.010885733911661, 3.774442382979625,\n    3.3687639483798324, 1.5506452040608503, 3.790447985799683, 3.8689707574953,\n    3.4933565751758713, 1.4284538820635841, 3.8715350915692364, 3.7045949061144983];\nscatter(x, y; xlabel=\"x\", ylabel=\"y\")","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Split the observations into train and test set.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"(x_train, y_train) = (x[begin:8], y[begin:8]);\n(x_test, y_test) = (x[9:end], y[9:end]);\nnothing #hide","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Instantiate the kernel.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"k = Matern52Kernel()","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Instantiate a Gaussian Process with the given kernel k.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"f = GP(k)","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Instantiate a FiniteGP, a finite dimentional projection at the inputs of the dataset observed under Gaussian Noise with sigma = 0001 .","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"fx = f(x_train, 0.001)","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Data's log-likelihood w.r.t prior GP.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"logpdf(fx, y_train)","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Calculating the exact posterior over f given y. The GP's kernel currently has some arbitrary fixed parameters.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"p_fx = posterior(fx, y_train)","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Data's log-likelihood under the posterior GP. We see that it drastically increases.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"logpdf(p_fx(x_test), y_test)","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Plot the posterior p_fx along with the observations.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"plt = scatter(x_train, y_train, label=\"Train Data\")\nscatter!(plt, x_test, y_test, label=\"Test Data\")\nplot!(plt, p_fx, 0:0.001:1; label=\"Posterior\")","category":"page"},{"location":"examples/HMC/#Hamiltonian-Monte-Carlo-Sampler","page":"Approximate Inference with NUTS-HMC","title":"Hamiltonian Monte Carlo Sampler","text":"","category":"section"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Previously we computed the log likelihood of the untuned kernel parameters of the GP, -1285. We now also perform approximate inference over said kernel parameters using the No-U-Turn Sampler - Hamiltonian Monte Carlo(NUTS-HMC) Sampler (paper) provided by AdvancedHMC.jl. We start of by loading necessary packages.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"using AdvancedHMC, Distributions, ForwardDiff","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"We define a function which returns log-probability of the data under the GP / log-likelihood of the parameters of the GP.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"function logp(params; x=x_train, y=y_train)\n    kernel = ScaledKernel(\n        transform(\n            Matern52Kernel(),\n            ScaleTransform(exp(params[1]))\n        ),\n        exp(params[2])\n    )\n    f = GP(kernel)\n    fx = f(x, 0.01)\n    return logpdf(fx, y)\nend","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Set the number of samples to draw and warmup iterations","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"n_samples, n_adapts = 2_000, 1_000\ninitial_params = rand(2)","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Define a Hamiltonian system","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"metric = DiagEuclideanMetric(2)\nhamiltonian = Hamiltonian(metric, logp, ForwardDiff)","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Define a leapfrog solver, with initial step size chosen heuristically","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"initial_ϵ = find_good_stepsize(hamiltonian, initial_params)\nintegrator = Leapfrog(initial_ϵ)","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Define an HMC sampler, with the following components","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"multinomial sampling scheme,\ngeneralised No-U-Turn criteria, and\nwindowed adaption for step-size and diagonal mass matrix","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"proposal = NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\nadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Run the sampler to draw samples from the specified Gaussian, where","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"samples will store the samples\nstats will store diagnostic statistics for each sample","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"samples, stats = sample(\n    hamiltonian,\n    proposal,\n    initial_params,\n    n_samples,\n    adaptor,\n    n_adapts;\n    progress=false\n)\nsamples_mat = reduce(hcat, samples)';\nnothing #hide","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Mean of samples of both the parameters.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"mean_params = mean(samples_mat; dims=1)","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Plotting a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"plt = histogram(samples_mat; layout=2, labels= \"Param\")\nvline!(plt, mean_params; layout=2, label=\"Mean\")","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Average log-marginal-probability of data with posterior kernel parameter samples sampled using ESS. We can observe that there is significant improvement over exact posterior with default kernel parameters.","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"mean(logp(param; x=x_test, y=y_test) for param in samples)","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"Plotting sampled functions from posterior with tuned parameters","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"plt = scatter(x_train, y_train, label=\"Train Data\")\nscatter!(plt, x_test, y_test, label=\"Test Data\")\nfor params in @view(samples[(end-100):end,:])\n    opt_kernel = ScaledKernel(\n        transform(\n            Matern52Kernel(),\n            ScaleTransform(exp(params[1]))\n        ),\n        exp(params[2])\n    )\n    f = GP(opt_kernel)\n    p_fx = posterior(f(x_train, 0.1), y_train)\n    sampleplot!(plt, p_fx(collect(0:0.02:1)), 1)\nend\nplt","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"","category":"page"},{"location":"examples/HMC/","page":"Approximate Inference with NUTS-HMC","title":"Approximate Inference with NUTS-HMC","text":"This page was generated using Literate.jl.","category":"page"}]
}
