<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · AbstractGPs.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">AbstractGPs.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>API</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/EllipticalSliceSampling/">Approximate Inference using ESS</a></li><li><a class="tocitem" href="../examples/HMC/">Approximate Inference with NUTS-HMC</a></li><li><a class="tocitem" href="../examples/VariationalInference/">Approximate Inference on Sparse GPs using VI</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/master/docs/src/api.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API"><a class="docs-heading-anchor" href="#API">API</a><a id="API-1"></a><a class="docs-heading-anchor-permalink" href="#API" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.AbstractGP" href="#AbstractGPs.AbstractGP"><code>AbstractGPs.AbstractGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">abstract type AbstractGP end</code></pre><p>Supertype for various Gaussian process (GP) types. A common interface is provided for interacting with each of these objects. See [1] for an overview of GPs.</p><p>[1] - C. E. Rasmussen and C. Williams. &quot;Gaussian processes for machine learning&quot;.  MIT Press. 2006.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/abstract_gp.jl#LL3-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.ConstMean" href="#AbstractGPs.ConstMean"><code>AbstractGPs.ConstMean</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ConstMean{T&lt;:Real} &lt;: MeanFunction</code></pre><p>Returns <code>c</code> everywhere.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/gp/mean_function.jl#LL20-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.CustomMean" href="#AbstractGPs.CustomMean"><code>AbstractGPs.CustomMean</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CustomMean{Tf} &lt;: MeanFunction</code></pre><p>A wrapper around whatever unary function you fancy. Must be able to be mapped over an <code>AbstractVector</code> of inputs.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/gp/mean_function.jl#LL32-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.FiniteGP" href="#AbstractGPs.FiniteGP"><code>AbstractGPs.FiniteGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">FiniteGP{Tf&lt;:AbstractGP, Tx&lt;:AbstractVector, TΣy}</code></pre><p>The finite-dimensional projection of the AbstractGP <code>f</code> at <code>x</code>. Assumed to be observed under Gaussian noise with zero mean and covariance matrix <code>Σ</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/finite_gp.jl#LL1-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.GP" href="#AbstractGPs.GP"><code>AbstractGPs.GP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">GP{Tm&lt;:MeanFunction, Tk&lt;:Kernel}</code></pre><p>A Gaussian Process (GP) with known <code>mean</code> and <code>kernel</code>. See e.g. [1] for an introduction.</p><p><strong>Zero Mean</strong></p><p>If only one argument is provided, assume the mean to be zero everywhere:</p><pre><code class="language-julia-repl">julia&gt; f = GP(Matern32Kernel());

julia&gt; x = randn(5);

julia&gt; mean(f(x)) == zeros(5)
true

julia&gt; cov(f(x)) == kernelmatrix(Matern32Kernel(), x)
true</code></pre><p><strong>Constant Mean</strong></p><p>If a <code>Real</code> is provided as the first argument, assume the mean function is constant with that value</p><pre><code class="language-julia-repl">julia&gt; f = GP(5.0, Matern32Kernel());

julia&gt; x = randn(5);

julia&gt; mean(f(x)) == 5.0 .* ones(5)
true

julia&gt; cov(f(x)) == kernelmatrix(Matern32Kernel(), x)
true</code></pre><p><strong>Custom Mean</strong></p><p>Provide an arbitrary function to compute the mean:</p><pre><code class="language-julia-repl">julia&gt; f = GP(x -&gt; sin(x) + cos(x / 2), Matern32Kernel());

julia&gt; x = randn(5);

julia&gt; mean(f(x)) == sin.(x) .+ cos.(x ./ 2)
true

julia&gt; cov(f(x)) == kernelmatrix(Matern32Kernel(), x)
true</code></pre><p>[1] - C. E. Rasmussen and C. Williams. &quot;Gaussian processes for machine learning&quot;.  MIT Press. 2006.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/gp/gp.jl#LL1-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.LatentFiniteGP" href="#AbstractGPs.LatentFiniteGP"><code>AbstractGPs.LatentFiniteGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LatentFiniteGP(fx&lt;:FiniteGP, lik)</code></pre><ul><li><code>fx</code> is a <code>FiniteGP</code>.</li><li><code>lik</code> is the log likelihood function which maps sample from f to corresposing </li></ul><p>conditional likelihood distributions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/latent_gp/latent_gp.jl#LL17-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.LatentGP" href="#AbstractGPs.LatentGP"><code>AbstractGPs.LatentGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LatentGP(f&lt;:GP, lik, Σy)</code></pre><ul><li><code>f</code> is a <code>AbstractGP</code>.</li><li><code>lik</code> is the log likelihood function which maps sample from f to corresposing </li></ul><p>conditional likelihood distributions.</p><ul><li><code>Σy</code> is the observation noise</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/latent_gp/latent_gp.jl#LL1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.ZeroMean" href="#AbstractGPs.ZeroMean"><code>AbstractGPs.ZeroMean</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ZeroMean{T&lt;:Real} &lt;: MeanFunction</code></pre><p>Returns <code>zero(T)</code> everywhere.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/gp/mean_function.jl#LL3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.approx_posterior-Tuple{VFE,AbstractGPs.FiniteGP,AbstractArray{var&quot;#s16&quot;,1} where var&quot;#s16&quot;&lt;:Real,AbstractGPs.FiniteGP}" href="#AbstractGPs.approx_posterior-Tuple{VFE,AbstractGPs.FiniteGP,AbstractArray{var&quot;#s16&quot;,1} where var&quot;#s16&quot;&lt;:Real,AbstractGPs.FiniteGP}"><code>AbstractGPs.approx_posterior</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">approx_posterior(::VFE, fx::FiniteGP, y::AbstractVector{&lt;:Real}, u::FiniteGP)</code></pre><p>Compute the optimal approximate posterior [1] over the process <code>f</code>, given observations <code>y</code> of <code>f</code> at <code>x</code>, and inducing points <code>u</code>, where <code>u = f(z)</code> for some inducing inputs <code>z</code>.</p><p>[1] - M. K. Titsias. &quot;Variational learning of inducing variables in sparse Gaussian processes&quot;. In: Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics. 2009.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/posterior_gp/approx_posterior_gp.jl#LL10-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.cov_diag-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}" href="#AbstractGPs.cov_diag-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}"><code>AbstractGPs.cov_diag</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">cov_diag(f::AbstractGP, x::AbstractVector)</code></pre><p>Compute only the diagonal elements of <code>cov(f(x))</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/abstract_gp.jl#LL28-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.dtc-Tuple{AbstractGPs.FiniteGP,AbstractArray{var&quot;#s15&quot;,1} where var&quot;#s15&quot;&lt;:Real,AbstractGPs.FiniteGP}" href="#AbstractGPs.dtc-Tuple{AbstractGPs.FiniteGP,AbstractArray{var&quot;#s15&quot;,1} where var&quot;#s15&quot;&lt;:Real,AbstractGPs.FiniteGP}"><code>AbstractGPs.dtc</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">dtc(f::FiniteGP, y::AbstractVector{&lt;:Real}, u::FiniteGP)</code></pre><p>The Deterministic Training Conditional (DTC) [1]. <code>y</code> are observations of <code>f</code>, and <code>u</code> are pseudo-points.</p><pre><code class="language-julia-repl">julia&gt; f = GP(Matern52Kernel());

julia&gt; x = randn(1000);

julia&gt; z = range(-5.0, 5.0; length=256);

julia&gt; y = rand(f(x, 0.1));

julia&gt; isapprox(dtc(f(x, 0.1), y, f(z)), logpdf(f(x, 0.1), y); atol=1e-3, rtol=1e-3)
true</code></pre><p>[1] - M. Seeger, C. K. I. Williams and N. D. Lawrence. &quot;Fast Forward Selection to Speed Up Sparse Gaussian Process Regression&quot;. In: Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics. 2003</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/finite_gp.jl#LL260-L283">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.elbo-Tuple{AbstractGPs.FiniteGP,AbstractArray{var&quot;#s13&quot;,1} where var&quot;#s13&quot;&lt;:Real,AbstractGPs.FiniteGP}" href="#AbstractGPs.elbo-Tuple{AbstractGPs.FiniteGP,AbstractArray{var&quot;#s13&quot;,1} where var&quot;#s13&quot;&lt;:Real,AbstractGPs.FiniteGP}"><code>AbstractGPs.elbo</code></a> — <span class="docstring-category">Method</span></header><section><div><p>elbo(f::FiniteGP, y::AbstractVector{&lt;:Real}, u::FiniteGP)</p><p>The Titsias Evidence Lower BOund (ELBO) [1]. <code>y</code> are observations of <code>f</code>, and <code>u</code> are pseudo-points, where <code>u = f(z)</code> for some <code>z</code>.</p><pre><code class="language-julia-repl">julia&gt; f = GP(Matern52Kernel());

julia&gt; x = randn(1000);

julia&gt; z = range(-5.0, 5.0; length=13);

julia&gt; y = rand(f(x, 0.1));

julia&gt; elbo(f(x, 0.1), y, f(z)) &lt; logpdf(f(x, 0.1), y)
true</code></pre><p>[1] - M. K. Titsias. &quot;Variational learning of inducing variables in sparse Gaussian processes&quot;. In: Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics. 2009.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/finite_gp.jl#LL231-L254">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.marginals-Tuple{AbstractGPs.FiniteGP}" href="#AbstractGPs.marginals-Tuple{AbstractGPs.FiniteGP}"><code>AbstractGPs.marginals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">marginals(f::FiniteGP)</code></pre><p>Compute a vector of Normal distributions representing the marginals of <code>f</code> efficiently. In particular, the off-diagonal elements of <code>cov(f(x))</code> are never computed.</p><pre><code class="language-julia-repl">julia&gt; f = GP(Matern32Kernel());

julia&gt; x = randn(11);

julia&gt; fs = marginals(f(x));

julia&gt; mean.(fs) == mean(f(x))
true

julia&gt; std.(fs) == sqrt.(diag(cov(f(x))))
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/finite_gp.jl#LL136-L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.mean_and_cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}" href="#AbstractGPs.mean_and_cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}"><code>AbstractGPs.mean_and_cov</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mean_and_cov(f::AbstractGP, x::AbstractVector)</code></pre><p>Compute both <code>mean(f(x))</code> and <code>cov(f(x))</code>. Sometimes more efficient than separately computation, particularly for posteriors.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/abstract_gp.jl#LL42-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.mean_and_cov-Tuple{AbstractGPs.FiniteGP}" href="#AbstractGPs.mean_and_cov-Tuple{AbstractGPs.FiniteGP}"><code>AbstractGPs.mean_and_cov</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mean_and_cov(f::FiniteGP)</code></pre><p>Equivalent to <code>(mean(f), cov(f))</code>, but sometimes more efficient to compute them jointly than separately.</p><pre><code class="language-julia-repl">julia&gt; fx = GP(SqExponentialKernel())(range(-3.0, 3.0; length=10), 0.1);

julia&gt; mean_and_cov(fx) == (mean(fx), cov(fx))
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/finite_gp.jl#LL95-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.mean_and_cov_diag-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}" href="#AbstractGPs.mean_and_cov_diag-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}"><code>AbstractGPs.mean_and_cov_diag</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mean_and_cov_diag(f::AbstractGP, x::AbstractVector)</code></pre><p>Compute both <code>mean(f(x))</code> and the diagonal elements of <code>cov(f(x))</code>. Sometimes more efficient than separately computation, particularly for posteriors.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/abstract_gp.jl#LL50-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.posterior-Tuple{AbstractGPs.FiniteGP,AbstractArray{var&quot;#s13&quot;,1} where var&quot;#s13&quot;&lt;:Real}" href="#AbstractGPs.posterior-Tuple{AbstractGPs.FiniteGP,AbstractArray{var&quot;#s13&quot;,1} where var&quot;#s13&quot;&lt;:Real}"><code>AbstractGPs.posterior</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">posterior(fx::FiniteGP, y::AbstractVector{&lt;:Real})</code></pre><p>Constructs the posterior distribution over <code>fx.f</code> given observations <code>y</code> at <code>x</code> made under noise <code>fx.Σy</code>. This is another <code>AbstractGP</code> object. See chapter 2 of [1] for a recap on exact inference in GPs. This posterior process has mean function</p><pre><code class="language-julia">m_posterior(x) = m(x) + k(x, fx.x) inv(cov(fx)) (y - mean(fx))</code></pre><p>and kernel</p><pre><code class="language-julia">k_posterior(x, z) = k(x, z) - k(x, fx.x) inv(cov(fx)) k(fx.x, z)</code></pre><p>where <code>m</code> and <code>k</code> are the mean function and kernel of <code>fx.f</code> respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/posterior_gp/posterior_gp.jl#LL6-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.posterior-Tuple{AbstractGPs.FiniteGP{var&quot;#s13&quot;,Tx,TΣ} where TΣ where Tx&lt;:(AbstractArray{T,1} where T) where var&quot;#s13&quot;&lt;:AbstractGPs.PosteriorGP,AbstractArray{var&quot;#s23&quot;,1} where var&quot;#s23&quot;&lt;:Real}" href="#AbstractGPs.posterior-Tuple{AbstractGPs.FiniteGP{var&quot;#s13&quot;,Tx,TΣ} where TΣ where Tx&lt;:(AbstractArray{T,1} where T) where var&quot;#s13&quot;&lt;:AbstractGPs.PosteriorGP,AbstractArray{var&quot;#s23&quot;,1} where var&quot;#s23&quot;&lt;:Real}"><code>AbstractGPs.posterior</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">posterior(fx::FiniteGP{&lt;:PosteriorGP}, y::AbstractVector{&lt;:Real})</code></pre><p>Constructs the posterior distribution over <code>fx.f</code> when <code>f</code> is itself a <code>PosteriorGP</code> by updating the cholesky factorisation of the covariance matrix and avoiding recomputing it from original covariance matrix. It does this by using <code>update_chol</code> functionality.</p><p>Other aspects are similar to a regular posterior.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/posterior_gp/posterior_gp.jl#LL29-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.sampleplot!-Tuple" href="#AbstractGPs.sampleplot!-Tuple"><code>AbstractGPs.sampleplot!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">sampleplot(GP::FiniteGP, samples)</code></pre><p>Plot samples from the given <code>FiniteGP</code>. Make sure to run <code>using Plots</code> before using this  function. </p><p><strong>Example</strong></p><pre><code class="language-julia">using Plots
f = GP(SqExponentialKernel())
sampleplot(f(rand(10)), 10; markersize=5)</code></pre><p>The given example plots 10 samples from the given <code>FiniteGP</code>. The <code>markersize</code> is modified from default of 0.5 to 5.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/539f3ce943f59dec8aff3f2238b083f1b27f41e5/base/#L0-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.sampleplot!-Tuple{RecipesBase.AbstractPlot,Vararg{Any,N} where N}" href="#AbstractGPs.sampleplot!-Tuple{RecipesBase.AbstractPlot,Vararg{Any,N} where N}"><code>AbstractGPs.sampleplot!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">sampleplot(GP::FiniteGP, samples)</code></pre><p>Plot samples from the given <code>FiniteGP</code>. Make sure to run <code>using Plots</code> before using this  function. </p><p><strong>Example</strong></p><pre><code class="language-julia">using Plots
f = GP(SqExponentialKernel())
sampleplot(f(rand(10)), 10; markersize=5)</code></pre><p>The given example plots 10 samples from the given <code>FiniteGP</code>. The <code>markersize</code> is modified from default of 0.5 to 5.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/539f3ce943f59dec8aff3f2238b083f1b27f41e5/base/#L0-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.sampleplot-Tuple" href="#AbstractGPs.sampleplot-Tuple"><code>AbstractGPs.sampleplot</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">sampleplot(GP::FiniteGP, samples)</code></pre><p>Plot samples from the given <code>FiniteGP</code>. Make sure to run <code>using Plots</code> before using this  function. </p><p><strong>Example</strong></p><pre><code class="language-julia">using Plots
f = GP(SqExponentialKernel())
sampleplot(f(rand(10)), 10; markersize=5)</code></pre><p>The given example plots 10 samples from the given <code>FiniteGP</code>. The <code>markersize</code> is modified from default of 0.5 to 5.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/539f3ce943f59dec8aff3f2238b083f1b27f41e5/base/#L0-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.update_approx_posterior-Tuple{AbstractGPs.ApproxPosteriorGP,AbstractGPs.FiniteGP,AbstractArray{var&quot;#s15&quot;,1} where var&quot;#s15&quot;&lt;:Real}" href="#AbstractGPs.update_approx_posterior-Tuple{AbstractGPs.ApproxPosteriorGP,AbstractGPs.FiniteGP,AbstractArray{var&quot;#s15&quot;,1} where var&quot;#s15&quot;&lt;:Real}"><code>AbstractGPs.update_approx_posterior</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function update_approx_posterior(
    f_post_approx::ApproxPosteriorGP,
    fx::FiniteGP,
    y::AbstractVector{&lt;:Real}
)</code></pre><p>Update the <code>ApproxPosteriorGP</code> given a new set of observations. Here, we retain the same  of pseudo-points.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/posterior_gp/approx_posterior_gp.jl#LL47-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.update_approx_posterior-Tuple{AbstractGPs.ApproxPosteriorGP,AbstractGPs.FiniteGP}" href="#AbstractGPs.update_approx_posterior-Tuple{AbstractGPs.ApproxPosteriorGP,AbstractGPs.FiniteGP}"><code>AbstractGPs.update_approx_posterior</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">function update_approx_posterior(
    f_post_approx::ApproxPosteriorGP,
    u::FiniteGP,
)</code></pre><p>Update the <code>ApproxPosteriorGP</code> given a new set of pseudo-points to append to the existing  set of pseudo points. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/posterior_gp/approx_posterior_gp.jl#LL99-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.update_chol-Tuple{LinearAlgebra.Cholesky,AbstractArray{T,2} where T,AbstractArray{T,2} where T}" href="#AbstractGPs.update_chol-Tuple{LinearAlgebra.Cholesky,AbstractArray{T,2} where T,AbstractArray{T,2} where T}"><code>AbstractGPs.update_chol</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia"> update_chol(chol::Cholesky, C12::AbstractMatrix, C22::AbstractMatrix)</code></pre><p>Let <code>C</code> be the positive definite matrix comprising blocks</p><pre><code class="language-julia">C = [C11 C12;
     C21 C22]</code></pre><p>with upper-triangular cholesky factorisation comprising blocks</p><pre><code class="language-julia">U = [U11 U12;
     0   U22]</code></pre><p>where <code>U11</code> and <code>U22</code> are themselves upper-triangular, and <code>U11 = cholesky(C11).U</code>. update_chol computes the updated Cholesky given original <code>chol</code>, <code>C12</code>, and <code>C22</code>.</p><p><strong>Arguments</strong></p><pre><code class="language-none"> - chol::Cholesky: The original cholesky decomposition
 - C12::AbstractMatrix: matrix of size (size(chol.U, 1), size(C22, 1))
 - C22::AbstractMatrix: positive-definite matrix</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/util/common_covmat_ops.jl#LL2-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.rand-Tuple{Random.AbstractRNG,AbstractGPs.FiniteGP,Int64}" href="#Base.rand-Tuple{Random.AbstractRNG,AbstractGPs.FiniteGP,Int64}"><code>Base.rand</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">rand(rng::AbstractRNG, f::FiniteGP, N::Int=1)</code></pre><p>Obtain <code>N</code> independent samples from the marginals <code>f</code> using <code>rng</code>. Single-sample methods produce a <code>length(f)</code> vector. Multi-sample methods produce a <code>length(f)</code> x <code>N</code> <code>Matrix</code>.</p><pre><code class="language-julia-repl">julia&gt; f = GP(Matern32Kernel());

julia&gt; x = randn(11);

julia&gt; rand(f(x)) isa Vector{Float64}
true

julia&gt; rand(MersenneTwister(123456), f(x)) isa Vector{Float64}
true

julia&gt; rand(f(x), 3) isa Matrix{Float64}
true

julia&gt; rand(MersenneTwister(123456), f(x), 3) isa Matrix{Float64}
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/finite_gp.jl#LL159-L183">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Distributions.logpdf-Tuple{AbstractGPs.FiniteGP,Union{AbstractArray{var&quot;#s16&quot;,1}, AbstractArray{var&quot;#s16&quot;,2}} where var&quot;#s16&quot;&lt;:Real}" href="#Distributions.logpdf-Tuple{AbstractGPs.FiniteGP,Union{AbstractArray{var&quot;#s16&quot;,1}, AbstractArray{var&quot;#s16&quot;,2}} where var&quot;#s16&quot;&lt;:Real}"><code>Distributions.logpdf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">logpdf(f::FiniteGP, y::AbstractVecOrMat{&lt;:Real})</code></pre><p>The logpdf of <code>y</code> under <code>f</code> if is <code>y isa AbstractVector</code>. logpdf of each column of <code>y</code> if <code>y isa Matrix</code>.</p><pre><code class="language-julia-repl">julia&gt; f = GP(Matern32Kernel());

julia&gt; x = randn(11);

julia&gt; y = rand(f(x));

julia&gt; logpdf(f(x), y) isa Real
true

julia&gt; Y = rand(f(x), 3);

julia&gt; logpdf(f(x), Y) isa AbstractVector{&lt;:Real}
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/finite_gp.jl#LL193-L215">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Distributions.logpdf-Tuple{AbstractGPs.LatentFiniteGP,NamedTuple{(:f, :y),T} where T&lt;:Tuple}" href="#Distributions.logpdf-Tuple{AbstractGPs.LatentFiniteGP,NamedTuple{(:f, :y),T} where T&lt;:Tuple}"><code>Distributions.logpdf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">logpdf(lfgp::LatentFiniteGP, y::NamedTuple{(:f, :y)})</code></pre><div>\[    log p(y, f; x)\]</div><p>Returns the joint log density of the gaussian process output <code>f</code> and real output <code>y</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/latent_gp/latent_gp.jl#LL38-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T,AbstractArray{T,1} where T}" href="#Statistics.cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T,AbstractArray{T,1} where T}"><code>Statistics.cov</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">cov(f::AbstractGP, x::AbstractVector, y::AbstractVector)</code></pre><p>Compute the <code>length(x)</code> by <code>length(y)</code> cross-covariance matrix between <code>f(x)</code> and <code>f(y)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/abstract_gp.jl#LL35-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}" href="#Statistics.cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}"><code>Statistics.cov</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">cov(f::AbstractGP, x::AbstractVector)</code></pre><p>Compute the <code>length(x)</code> by <code>length(x)</code> covariance matrix of the multivariate Normal <code>f(x)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/abstract_gp.jl#LL21-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.cov-Tuple{AbstractGPs.FiniteGP,AbstractGPs.FiniteGP}" href="#Statistics.cov-Tuple{AbstractGPs.FiniteGP,AbstractGPs.FiniteGP}"><code>Statistics.cov</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">cov(fx::FiniteGP, gx::FiniteGP)</code></pre><p>Compute the cross-covariance matrix between <code>fx</code> and <code>gx</code>.</p><pre><code class="language-julia-repl">julia&gt; f = GP(Matern32Kernel());

julia&gt; x1 = randn(11);

julia&gt; x2 = randn(13);

julia&gt; cov(f(x1), f(x2)) == kernelmatrix(Matern32Kernel(), x1, x2)
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/finite_gp.jl#LL114-L130">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.cov-Tuple{AbstractGPs.FiniteGP}" href="#Statistics.cov-Tuple{AbstractGPs.FiniteGP}"><code>Statistics.cov</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">cov(f::FiniteGP)</code></pre><p>Compute the covariance matrix of <code>fx</code>.</p><p><strong>Noise-free observations</strong></p><pre><code class="language-julia-repl">julia&gt; f = GP(Matern52Kernel());

julia&gt; x = randn(11);

julia&gt; cov(f(x)) == kernelmatrix(Matern52Kernel(), x)
true</code></pre><p><strong>Isotropic observation noise</strong></p><pre><code class="language-julia-repl">julia&gt; cov(f(x, 0.1)) == kernelmatrix(Matern52Kernel(), x) + 0.1 * I
true</code></pre><p><strong>Independent anisotropic observation noise</strong></p><pre><code class="language-julia-repl">julia&gt; s = rand(11);

julia&gt; cov(f(x, s)) == kernelmatrix(Matern52Kernel(), x) + Diagonal(s)
true</code></pre><p><strong>Correlated observation noise</strong></p><pre><code class="language-julia-repl">julia&gt; A = randn(11, 11); S = A&#39;A;

julia&gt; cov(f(x, S)) == kernelmatrix(Matern52Kernel(), x) + S
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/finite_gp.jl#LL51-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.mean-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}" href="#Statistics.mean-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}"><code>Statistics.mean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mean(f::AbstractGP, x::AbstractVector)</code></pre><p>Computes the mean vector of the multivariate Normal <code>f(x)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/abstract_gp.jl#LL14-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.mean-Tuple{AbstractGPs.FiniteGP}" href="#Statistics.mean-Tuple{AbstractGPs.FiniteGP}"><code>Statistics.mean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mean(fx::FiniteGP)</code></pre><p>Compute the mean vector of <code>fx</code>.</p><pre><code class="language-julia-repl">julia&gt; f = GP(Matern52Kernel());

julia&gt; x = randn(11);

julia&gt; mean(f(x)) == zeros(11)
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/21a8eba772d4cd74931ffca0ca644e98611bd584/src/abstract_gp/finite_gp.jl#LL34-L48">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../examples/EllipticalSliceSampling/">Approximate Inference using ESS »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 6 November 2020 14:40">Friday 6 November 2020</span>. Using Julia version 1.5.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
