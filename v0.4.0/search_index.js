var documenterSearchIndex = {"docs":
[{"location":"api/#FiniteGP-and-AbstractGP","page":"The Main APIs","title":"FiniteGP and AbstractGP","text":"","category":"section"},{"location":"api/#Intended-Audience","page":"The Main APIs","title":"Intended Audience","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"This page is intended for developers. If you are a user, please refer to our other examples.","category":"page"},{"location":"api/#Introduction","page":"The Main APIs","title":"Introduction","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"AbstractGPs provides the abstract type AbstractGP, and the concrete type FiniteGP. An AbstractGP, f, should be thought of as a distribution over functions. This means that the output of rand(f) would be a real-valued function. It's not usually possible to implement this though, so we don't.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"A FiniteGP fx = f(x) represents the distribution over functions at the finite collection of points specified in x. fx is a multivariate Normal distribution, so rand(fx) produces a Vector of Reals.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"A FiniteGP is the interesting object computationally, so if you create a new subtype MyNewGP of AbstractGP, and wish to make it interact well with the rest of the GP ecosystem, the methods that you must implement are not those directly involving MyNewGP, but rather those involving","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"FiniteGP{<:MyNewGP}","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"We provide two ways in which to do this. The first is to implement methods directly on Finite{<:MyNewGP} – this is detailed in the FiniteGP APIs. The second is to implement some methods directly involving MyNewGP, and utilise default FiniteGP methods implemented in terms of these – this is detailed in the Internal AbstractGPs API. For example, the first method involves implementing methods like AbstractGPs.mean(fx::FiniteGP{<:MyNewGP}), while the second involves AbstractGPs.mean(f::MyNewGP, x::AbstractVector).","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"The second interface is generally easier to implement, but isn't always the best choice. See Which API should I implement? for further discussion.","category":"page"},{"location":"api/#FiniteGP-APIs","page":"The Main APIs","title":"FiniteGP APIs","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"Let f be an AbstractGP, x an AbstractVector representing a collection of inputs, and Σ a positive-definite matrix of size (length(x), length(x)). A FiniteGP represents the multivariate Gaussian induced by \"indexing\" into f at each point in x, and adding independent zero-mean noise with covariance matrix Σ:","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"fx = f(x, Σ)\n\n# The code below is equivalent to the above, and is just for reference.\n# When writing code, prefer the above syntax.\nfx = AbstractGPs.FiniteGP(f, x, Σ)","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"The FiniteGP has two API levels. The Primary Public API should be supported by all FiniteGPs, while the Secondary Public API will only be supported by a subset. Use only the primary API when possible.","category":"page"},{"location":"api/#Primary-Public-API","page":"The Main APIs","title":"Primary Public API","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"These are user-facing methods. You can expect them to be implemented whenever you encounter a FiniteGP. If you are building something on top of AbstractGPs, try to implement it in terms of these functions.","category":"page"},{"location":"api/#Required-Methods","page":"The Main APIs","title":"Required Methods","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"rand\nrand!\nmarginals\nlogpdf(::AbstractGPs.FiniteGP, ::AbstractVector{<:Real})\nposterior(::AbstractGPs.FiniteGP, ::AbstractVector{<:Real})\nmean(::AbstractGPs.FiniteGP)\nvar(::AbstractGPs.FiniteGP)","category":"page"},{"location":"api/#Base.rand","page":"The Main APIs","title":"Base.rand","text":"rand(rng::AbstractRNG, f::FiniteGP, N::Int=1)\n\nObtain N independent samples from the marginals f using rng. Single-sample methods produce a length(f) vector. Multi-sample methods produce a length(f) × N Matrix.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(11);\n\njulia> rand(f(x)) isa Vector{Float64}\ntrue\n\njulia> rand(MersenneTwister(123456), f(x)) isa Vector{Float64}\ntrue\n\njulia> rand(f(x), 3) isa Matrix{Float64}\ntrue\n\njulia> rand(MersenneTwister(123456), f(x), 3) isa Matrix{Float64}\ntrue\n\n\n\n\n\n","category":"function"},{"location":"api/#Random.rand!","page":"The Main APIs","title":"Random.rand!","text":"rand!(rng::AbstractRNG, f::FiniteGP, y::AbstractVecOrMat{<:Real})\n\nObtain sample(s) from the marginals f using rng and write them to y.\n\nIf y is a matrix, then each column corresponds to an independent sample.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(11);\n\njulia> y = similar(x);\n\njulia> rand!(f(x), y);\n\njulia> rand!(MersenneTwister(123456), f(x), y);\n\njulia> ys = similar(x, length(x), 3);\n\njulia> rand!(f(x), ys);\n\njulia> rand!(MersenneTwister(123456), f(x), ys);\n\n\n\n\n\n","category":"function"},{"location":"api/#AbstractGPs.marginals","page":"The Main APIs","title":"AbstractGPs.marginals","text":"marginals(f::FiniteGP)\n\nCompute a vector of Normal distributions representing the marginals of f efficiently. In particular, the off-diagonal elements of cov(f(x)) are never computed.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(11);\n\njulia> fs = marginals(f(x));\n\njulia> mean.(fs) == mean(f(x))\ntrue\n\njulia> std.(fs) == sqrt.(diag(cov(f(x))))\ntrue\n\n\n\n\n\n","category":"function"},{"location":"api/#Distributions.logpdf-Tuple{AbstractGPs.FiniteGP, AbstractVector{var\"#s2\"} where var\"#s2\"<:Real}","page":"The Main APIs","title":"Distributions.logpdf","text":"logpdf(f::FiniteGP, y::AbstractVecOrMat{<:Real})\n\nThe logpdf of y under f if y isa AbstractVector. The logpdf of each column of y if y isa Matrix.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(11);\n\njulia> y = rand(f(x));\n\njulia> logpdf(f(x), y) isa Real\ntrue\n\njulia> Y = rand(f(x), 3);\n\njulia> logpdf(f(x), Y) isa AbstractVector{<:Real}\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.posterior-Tuple{AbstractGPs.FiniteGP, AbstractVector{var\"#s2\"} where var\"#s2\"<:Real}","page":"The Main APIs","title":"AbstractGPs.posterior","text":"posterior(fx::FiniteGP, y::AbstractVector{<:Real})\n\nConstruct the posterior distribution over fx.f given observations y at x made under noise fx.Σy. This is another AbstractGP object. See chapter 2 of [1] for a recap on exact inference in GPs. This posterior process has mean function\n\nm_posterior(x) = m(x) + k(x, fx.x) inv(cov(fx)) (y - mean(fx))\n\nand kernel\n\nk_posterior(x, z) = k(x, z) - k(x, fx.x) inv(cov(fx)) k(fx.x, z)\n\nwhere m and k are the mean function and kernel of fx.f, respectively.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{AbstractGPs.FiniteGP}","page":"The Main APIs","title":"Statistics.mean","text":"mean(fx::FiniteGP)\n\nCompute the mean vector of fx.\n\njulia> f = GP(Matern52Kernel());\n\njulia> x = randn(11);\n\njulia> mean(f(x)) == zeros(11)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.var-Tuple{AbstractGPs.FiniteGP}","page":"The Main APIs","title":"Statistics.var","text":"var(f::FiniteGP)\n\nCompute only the diagonal elements of cov(f).\n\nExamples\n\njulia> fx = GP(Matern52Kernel())(randn(10), 0.1);\n\njulia> var(fx) == diag(cov(fx))\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Optional-methods","page":"The Main APIs","title":"Optional methods","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"Default implementations are provided for these, but you may wish to specialise for performance.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"mean_and_var(::AbstractGPs.FiniteGP)","category":"page"},{"location":"api/#StatsBase.mean_and_var-Tuple{AbstractGPs.FiniteGP}","page":"The Main APIs","title":"StatsBase.mean_and_var","text":"mean_and_var(f::FiniteGP)\n\nCompute both mean(f) and the diagonal elements of cov(f).\n\nSometimes more efficient than computing them separately, particularly for posteriors.\n\nExamples\n\njulia> fx = GP(SqExponentialKernel())(range(-3.0, 3.0; length=10), 0.1);\n\njulia> mean_and_var(fx) == (mean(fx), var(fx))\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Secondary-Public-API","page":"The Main APIs","title":"Secondary Public API","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"Observe that the Primary Public API does not include a function to compute the covariance matrix of a FiniteGP. While the covariance matrix of any multivariate Gaussian is defined, it is not always a good idea to actually compute it. Fortunately, it's often the case that you're not actually interested in the covariance matrix per-se, rather the other quantities that you might use it to compute (logpdf, rand, posterior). This is similar to the well-known observation that you rarely need the inverse of a matrix, you just need to compute the inverse multiplied by something, so it's considered good practice to avoid ever explicitly computing the inverse of a matrix so as to avoid the numerical issues associated with it. This is important, for example, as TemporalGPs.jl is able to implement all of the Primary Public API in linear time in the dimension of the FiniteGP, as it never needs to evaluate the covariance matrix.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"However, for many (probably the majority of) GPs, this acceleration isn't possible, and there is really nothing lost by explicitly evaluating the covariance matrix. We call this the Secondary Public API, because it's available a large proportion of the time, but should be avoided if at all possible.","category":"page"},{"location":"api/#Required-Methods-2","page":"The Main APIs","title":"Required Methods","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"cov(::AbstractGPs.FiniteGP)","category":"page"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.FiniteGP}","page":"The Main APIs","title":"Statistics.cov","text":"cov(f::FiniteGP)\n\nCompute the covariance matrix of fx.\n\nNoise-free observations\n\njulia> f = GP(Matern52Kernel());\n\njulia> x = randn(11);\n\njulia> cov(f(x)) == kernelmatrix(Matern52Kernel(), x)\ntrue\n\nIsotropic observation noise\n\njulia> cov(f(x, 0.1)) == kernelmatrix(Matern52Kernel(), x) + 0.1 * I\ntrue\n\nIndependent anisotropic observation noise\n\njulia> s = rand(11);\n\njulia> cov(f(x, s)) == kernelmatrix(Matern52Kernel(), x) + Diagonal(s)\ntrue\n\nCorrelated observation noise\n\njulia> A = randn(11, 11); S = A'A;\n\njulia> cov(f(x, S)) == kernelmatrix(Matern52Kernel(), x) + S\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Optional-Methods","page":"The Main APIs","title":"Optional Methods","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"Default implementations are provided for these, but you may wish to specialise for performance.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"mean_and_cov(::AbstractGPs.FiniteGP)","category":"page"},{"location":"api/#StatsBase.mean_and_cov-Tuple{AbstractGPs.FiniteGP}","page":"The Main APIs","title":"StatsBase.mean_and_cov","text":"mean_and_cov(f::FiniteGP)\n\nEquivalent to (mean(f), cov(f)), but sometimes more efficient to compute them jointly than separately.\n\njulia> fx = GP(SqExponentialKernel())(range(-3.0, 3.0; length=10), 0.1);\n\njulia> mean_and_cov(fx) == (mean(fx), cov(fx))\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Internal-AbstractGPs-API","page":"The Main APIs","title":"Internal AbstractGPs API","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"This functionality is not intended to be used directly by the users, or those building functionality on top of this package – they should interact with Primary Public API. If you believe you really do need to interact with this level of the API, please open an issue to discuss as you may have a use-case that was missed during the design of this API.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"As discussed at the top of this page, instances of subtypes of AbstractGP represent Gaussian processes – collections of jointly-Gaussian random variables, which may be infinite-dimensional.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"Implementing the following API for your own AbstractGP subtype automatically implements both the Primary and Secondary public APIs above in terms of them.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"Existing implementations of this interface include","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"GP\nPosteriorGP\nApproxPosteriorGP\nWrappedGP\nCompositeGP\nGaussianProcessProbabilisticProgramme","category":"page"},{"location":"api/#Required-Methods-3","page":"The Main APIs","title":"Required Methods","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"mean(::AbstractGPs.AbstractGP, ::AbstractVector)\ncov(::AbstractGPs.AbstractGP, ::AbstractVector, ::AbstractVector)\nvar(::AbstractGPs.AbstractGP, ::AbstractVector)","category":"page"},{"location":"api/#Statistics.mean-Tuple{AbstractGPs.AbstractGP, AbstractVector{T} where T}","page":"The Main APIs","title":"Statistics.mean","text":"mean(f::AbstractGP, x::AbstractVector)\n\nCompute the mean vector of the multivariate Normal f(x).\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.AbstractGP, AbstractVector{T} where T, AbstractVector{T} where T}","page":"The Main APIs","title":"Statistics.cov","text":"cov(f::AbstractGP, x::AbstractVector, y::AbstractVector)\n\nCompute the length(x) by length(y) cross-covariance matrix between f(x) and f(y).\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.var-Tuple{AbstractGPs.AbstractGP, AbstractVector{T} where T}","page":"The Main APIs","title":"Statistics.var","text":"var(f::AbstractGP, x::AbstractVector)\n\nCompute only the diagonal elements of cov(f(x)).\n\n\n\n\n\n","category":"method"},{"location":"api/#Optional-Methods-2","page":"The Main APIs","title":"Optional Methods","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"Default implementations are provided for these, but you may wish to specialise for performance.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"cov(::AbstractGPs.AbstractGP, ::AbstractVector)\nmean_and_cov(::AbstractGPs.AbstractGP, ::AbstractVector)\nmean_and_var(::AbstractGPs.AbstractGP, ::AbstractVector)","category":"page"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.AbstractGP, AbstractVector{T} where T}","page":"The Main APIs","title":"Statistics.cov","text":"cov(f::AbstractGP, x::AbstractVector)\n\nCompute the length(x) by length(x) covariance matrix of the multivariate Normal f(x).\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.mean_and_cov-Tuple{AbstractGPs.AbstractGP, AbstractVector{T} where T}","page":"The Main APIs","title":"StatsBase.mean_and_cov","text":"mean_and_cov(f::AbstractGP, x::AbstractVector)\n\nCompute both mean(f(x)) and cov(f(x)). Sometimes more efficient than computing them separately, particularly for posteriors.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.mean_and_var-Tuple{AbstractGPs.AbstractGP, AbstractVector{T} where T}","page":"The Main APIs","title":"StatsBase.mean_and_var","text":"mean_and_var(f::AbstractGP, x::AbstractVector)\n\nCompute both mean(f(x)) and the diagonal elements of cov(f(x)). Sometimes more efficient than computing them separately, particularly for posteriors.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"Note that, while we could provide a default implementation for var(f, x) as diag(cov(f, x)), this is generally such an inefficient fallback, that we find it preferable to error if it's not implemented than to ever hit a fallback.","category":"page"},{"location":"api/#Which-API-should-I-implement?","page":"The Main APIs","title":"Which API should I implement?","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"To answer this question, you need to need to know whether or not the default implementations of the FiniteGP APIs work for your use case. There are a couple of reasons of which we are aware for why this might not be the case (see below) – possibly there are others.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"If you are unsure, please open an issue to discuss.","category":"page"},{"location":"api/#You-want-to-avoid-computing-the-covariance-matrix","page":"The Main APIs","title":"You want to avoid computing the covariance matrix","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"We've already discussed this a bit on this page. The default implementations of the FiniteGP APIs rely on computing the covariance matrix. If your AbstractGP subtype needs to avoid computing the covariance matrix for performance reasons, then do not implement the Internal AbstractGPs API. Do implement the Primary Public API. Do not implement the Secondary Public API.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"TemporalGPs.jl is an example of a package that does this – see the LTISDE implementation for an example. The same is true of the BayesianLinearRegressor type.","category":"page"},{"location":"api/#You-don't-want-to-use-the-default-implementations","page":"The Main APIs","title":"You don't want to use the default implementations","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"Perhaps you just don't like the default implementations because you don't want to make use of Cholesky factorisations. We don't have an example of this yet in Julia, however GPyTorch avoids the Cholesky factorisation in favour of iterative solvers.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"In this situation, implement both the Internal AbstractGPs API and the FiniteGP APIs.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"In this situation you will benefit less from code reuse inside AbstractGPs, but will continue to benefit from the ability of others use your code, and to take advantage of any existing functionality which requires types which adhere to the AbstractGPs API.","category":"page"},{"location":"api/#Testing-Utilities","page":"The Main APIs","title":"Testing Utilities","text":"","category":"section"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"AbstractGPs provides several consistency tests in the AbstractGPs.TestUtils module. These tests will ensure that, for example, the size and type of everything produced by an implementation of the API is correct, and consistent with the other methods. It will not ensure correctness in any absolute sense though (e.g. that logpdf indeed computes what you wanted it to compute). Consequently, these tests should be seen as a set of necessary conditions for your code to be correct. They are not, however, sufficient.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"You should only need to run one of the following test suites.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"If you implement the Primary Public API, run test_finitegp_primary_public_interface.\nIf you implement both the Primary Public API and the Secondary Public API, then run test_finitegp_primary_and_secondary_public_interface.\nIf you implement the Internal AbstractGPs API, run test_internal_abstractgps_interface.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"Also see Which API should I implement? for more information about the most appropriate API to implement.","category":"page"},{"location":"api/","page":"The Main APIs","title":"The Main APIs","text":"AbstractGPs.TestUtils.test_finitegp_primary_public_interface\nAbstractGPs.TestUtils.test_finitegp_primary_and_secondary_public_interface\nAbstractGPs.TestUtils.test_internal_abstractgps_interface","category":"page"},{"location":"api/#AbstractGPs.TestUtils.test_finitegp_primary_public_interface","page":"The Main APIs","title":"AbstractGPs.TestUtils.test_finitegp_primary_public_interface","text":"test_finitegp_primary_public_interface(\n    rng::AbstractRNG, fx::FiniteGP; atol::Real=1e-12\n)\n\nBasic consistency tests for the Primary Public FiniteGP API. You should run these tests if you only implement the Primary Public API – see API section of docs for details about this API.\n\nThese are consistency checks, not correctness tests in the absolute sense. For example, these tests ensure that samples generated by rand are of the correct size, but does not check that they come from the intended distribution.\n\n\n\n\n\n","category":"function"},{"location":"api/#AbstractGPs.TestUtils.test_finitegp_primary_and_secondary_public_interface","page":"The Main APIs","title":"AbstractGPs.TestUtils.test_finitegp_primary_and_secondary_public_interface","text":"test_finitegp_primary_and_secondary_public_interface(\n    rng::AbstractRNG, fx::FiniteGP; atol::Real=1e-12\n)\n\nBasic consistency tests for both the Primary and Secondary Public FiniteGP APIs. Runs test_finitegp_primary_public_interface as part of these tests. You should run these tests if you implement both the primary and secondary public APIs – see API section of the docs for more information about these APIs.\n\nThese are consistency checks, not correctness tests in the absolute sense. For example, these tests ensure that samples generated by rand are of the correct size, but does not check that they come from the intended distribution.\n\n\n\n\n\n","category":"function"},{"location":"api/#AbstractGPs.TestUtils.test_internal_abstractgps_interface","page":"The Main APIs","title":"AbstractGPs.TestUtils.test_internal_abstractgps_interface","text":"test_internal_abstractgps_interface(\n    rng::AbstractRNG,\n    f::AbstractGP,\n    x::AbstractVector,\n    z::AbstractVector;\n    atol=1e-12,\n    σ²::Real=1e-9,\n)\n\nBasic consistency tests for the Internal AbstractGPs API. Runs test_finitegp_primary_and_secondary_public_interface as part of these tests. Run these tests if you implement the Internal AbstractGPs API – see the API section of the docs for more information about this API.\n\nThese are consistency checks, not correctness tests in the absolute sense. For example, these tests ensure that samples generated by rand are of the correct size, but does not check that they come from the intended distribution.\n\n\n\n\n\n","category":"function"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/master/examples/regression-1d/script.jl\"","category":"page"},{"location":"examples/regression-1d/#One-dimensional-regression","page":"One-dimensional regression","title":"One-dimensional regression","text":"","category":"section"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/regression-1d/#Setup","page":"One-dimensional regression","title":"Setup","text":"","category":"section"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Loading the necessary packages and setting seed.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"using AbstractGPs\nusing Distributions\nusing StatsFuns\n\nusing Plots\ndefault(; legend=:outertopright, size=(700, 400))\n\nusing Random\nRandom.seed!(1234)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Load toy regression dataset taken from GPFlow examples.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"x = [\n    0.8658165855998895,\n    0.6661700880180962,\n    0.8049218148148531,\n    0.7714303440386239,\n    0.14790478354654835,\n    0.8666105548197428,\n    0.007044577166530286,\n    0.026331737288148638,\n    0.17188596617099916,\n    0.8897812990554013,\n    0.24323574561119998,\n    0.028590102134105955,\n]\ny = [\n    1.5255314337144372,\n    3.6434202968230003,\n    3.010885733911661,\n    3.774442382979625,\n    3.3687639483798324,\n    1.5506452040608503,\n    3.790447985799683,\n    3.8689707574953,\n    3.4933565751758713,\n    1.4284538820635841,\n    3.8715350915692364,\n    3.7045949061144983,\n]\nscatter(x, y; xlabel=\"x\", ylabel=\"y\", legend=false)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We split the observations into train and test data.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"x_train = x[1:8]\ny_train = y[1:8]\nx_test = x[9:end]\ny_test = y[9:end]","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We instantiate a Gaussian process with a Matern kernel. The kernel has fixed variance and length scale parameters of default value 1.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"f = GP(Matern52Kernel())","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We create a finite dimentional projection at the inputs of the training dataset observed under Gaussian noise with standard deviation sigma = 01, and compute the log-likelihood of the outputs of the training dataset.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"fx = f(x_train, 0.1)\nlogpdf(fx, y_train)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"-25.53057444906228","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We compute the posterior Gaussian process given the training data, and calculate the log-likelihood of the test dataset.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"p_fx = posterior(fx, y_train)\nlogpdf(p_fx(x_test), y_test)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"-232.51565975767468","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We plot the posterior Gaussian process along with the observations.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"scatter(\n    x_train,\n    y_train;\n    xlim=(0, 1),\n    xlabel=\"x\",\n    ylabel=\"y\",\n    title=\"posterior (default parameters)\",\n    label=\"Train Data\",\n)\nscatter!(x_test, y_test; label=\"Test Data\")\nplot!(0:0.001:1, p_fx; label=false)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/#Markov-Chain-Monte-Carlo","page":"One-dimensional regression","title":"Markov Chain Monte Carlo","text":"","category":"section"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Previously we computed the log likelihood of the untuned kernel parameters of the GP. We now also perform approximate inference over said kernel parameters using different Markov chain Monte Carlo (MCMC) methods. I.e., we approximate the posterior distribution of the kernel parameters with samples from a Markov chain.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We define a function which returns the log-likelihood of the data for different variance and inverse lengthscale parameters of the Matern kernel. We ensure that these parameters are positive with the softplus function","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"f(x) = log (1 + exp x)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"function gp_loglikelihood(x, y)\n    function loglikelihood(params)\n        kernel =\n            softplus(params[1]) * (Matern52Kernel() ∘ ScaleTransform(softplus(params[2])))\n        f = GP(kernel)\n        fx = f(x, 0.1)\n        return logpdf(fx, y)\n    end\n    return loglikelihood\nend\n\nconst loglik_train = gp_loglikelihood(x_train, y_train)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We define a Gaussian prior for the joint distribution of the two transformed kernel parameters. We assume that both parameters are independent with mean 0 and variance 1.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"logprior(params) = logpdf(MvNormal(2, 1), params)","category":"page"},{"location":"examples/regression-1d/#Hamiltonian-Monte-Carlo","page":"One-dimensional regression","title":"Hamiltonian Monte Carlo","text":"","category":"section"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We start with a Hamiltonian Monte Carlo (HMC) sampler. More precisely, we use the No-U-Turn sampler (NUTS), which is provided by the Julia packages AdvancedHMC.jl and DynamicHMC.jl.","category":"page"},{"location":"examples/regression-1d/#AdvancedHMC","page":"One-dimensional regression","title":"AdvancedHMC","text":"","category":"section"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We start with performing inference with AdvancedHMC.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"using AdvancedHMC\nusing ForwardDiff","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Set the number of samples to draw and warmup iterations.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"n_samples = 2_000\nn_adapts = 1_000","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Define a Hamiltonian system of the log joint probability.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"logjoint_train(params) = loglik_train(params) + logprior(params)\nmetric = DiagEuclideanMetric(2)\nhamiltonian = Hamiltonian(metric, logjoint_train, ForwardDiff)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Define a leapfrog solver, with initial step size chosen heuristically.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"initial_params = rand(2)\ninitial_ϵ = find_good_stepsize(hamiltonian, initial_params)\nintegrator = Leapfrog(initial_ϵ)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Define an HMC sampler, with the following components:","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"multinomial sampling scheme,\ngeneralised No-U-Turn criteria, and\nwindowed adaption for step-size and diagonal mass matrix","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"proposal = NUTS{MultinomialTS,GeneralisedNoUTurn}(integrator)\nadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We draw samples from the posterior distribution of kernel parameters. These samples are in the unconstrained space mathbbR^2.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples, _ = sample(\n    hamiltonian, proposal, initial_params, n_samples, adaptor, n_adapts; progress=false\n)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"┌ Info: Finished 1000 adapation steps\n│   adaptor =\n│    StanHMCAdaptor(\n│        pc=WelfordVar,\n│        ssa=NesterovDualAveraging(γ=0.05, t_0=10.0, κ=0.75, δ=0.8, state.ϵ=0.8632215254591531),\n│        init_buffer=75, term_buffer=50, window_size=25,\n│        state=window(76, 950), window_splits(100, 150, 250, 450, 950)\n│    )\n│   τ.integrator = Leapfrog(ϵ=0.863)\n└   h.metric = DiagEuclideanMetric([0.4205557129728238, 0.4163 ...])\n┌ Info: Finished 2000 sampling steps for 1 chains in 2.514659706 (s)\n│   h = Hamiltonian(metric=DiagEuclideanMetric([0.4205557129728238, 0.4163 ...]))\n│   τ = NUTS{MultinomialTS,Generalised}(integrator=Leapfrog(ϵ=0.863), max_depth=10), Δ_max=1000.0)\n│   EBFMI_est = 1.143201310054058\n└   average_acceptance_rate = 0.8561286461510924\n","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We transform the samples back to the constrained space and compute the mean of both parameters:","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples_constrained = [map(softplus, p) for p in samples]\nmean_samples = mean(samples_constrained)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"2-element Vector{Float64}:\n 2.3256789662018496\n 2.264465149411044","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We plot a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"histogram(\n    reduce(hcat, samples_constrained)';\n    xlabel=\"sample\",\n    ylabel=\"counts\",\n    layout=2,\n    title=[\"variance\" \"inverse length scale\"],\n    legend=false,\n)\nvline!(mean_samples'; linewidth=2)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We approximate the log-likelihood of the test data using the posterior Gaussian processes for kernels with the sampled kernel parameters. We can observe that there is a significant improvement over the log-likelihood of the test data with respect to the posterior Gaussian process with default kernel parameters of value 1.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"function gp_posterior(x, y, p)\n    kernel = softplus(p[1]) * (Matern52Kernel() ∘ ScaleTransform(softplus(p[2])))\n    f = GP(kernel)\n    return posterior(f(x, 0.1), y)\nend\n\nmean(logpdf(gp_posterior(x_train, y_train, p)(x_test), y_test) for p in samples)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"-7.681537641630063","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We sample a function from the posterior GP for the final 100 samples of kernel parameters.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"plt = scatter(\n    x_train,\n    y_train;\n    xlim=(0, 1),\n    xlabel=\"x\",\n    ylabel=\"y\",\n    title=\"posterior (AdvancedHMC)\",\n    label=\"Train Data\",\n)\nscatter!(plt, x_test, y_test; label=\"Test Data\")\nfor p in samples[(end - 100):end]\n    sampleplot!(plt, 0:0.02:1, gp_posterior(x_train, y_train, p))\nend\nplt","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/#DynamicHMC","page":"One-dimensional regression","title":"DynamicHMC","text":"","category":"section"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We repeat the inference with DynamicHMC. DynamicHMC requires us to implement the LogDensityProblems interface for loglik_train.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"using DynamicHMC\nusing LogDensityProblems\n\n# Log joint density\nfunction LogDensityProblems.logdensity(ℓ::typeof(loglik_train), params)\n    return ℓ(params) + logprior(params)\nend\n\n# The parameter space is two-dimensional\nLogDensityProblems.dimension(::typeof(loglik_train)) = 2\n\n# `loglik_train` does not allow to evaluate derivatives of\n# the log-likelihood function\nfunction LogDensityProblems.capabilities(::Type{<:typeof(loglik_train)})\n    return LogDensityProblems.LogDensityOrder{0}()\nend","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Now we can draw samples from the posterior distribution of kernel parameters with DynamicHMC. Again we use ForwardDiff.jl to compute the derivatives of the log joint density with automatic differentiation.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples =\n    mcmc_with_warmup(\n        Random.GLOBAL_RNG,\n        ADgradient(:ForwardDiff, loglik_train),\n        n_samples;\n        reporter=NoProgressReport(),\n    ).chain","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We transform the samples back to the constrained space and compute the mean of both parameters:","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples_constrained = [map(softplus, p) for p in samples]\nmean_samples = mean(samples_constrained)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"2-element Vector{Float64}:\n 2.3108097020199474\n 2.2843742554786686","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We plot a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"histogram(\n    reduce(hcat, samples_constrained)';\n    xlabel=\"sample\",\n    ylabel=\"counts\",\n    layout=2,\n    title=[\"variance\" \"inverse length scale\"],\n    legend=false,\n)\nvline!(mean_samples'; linewidth=2)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Again we can observe that there is a significant improvement over the log-likelihood of the test data with respect to the posterior Gaussian process with default kernel parameters.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"mean(logpdf(gp_posterior(x_train, y_train, p)(x_test), y_test) for p in samples)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"-8.107428208367516","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We sample a function from the posterior GP for the final 100 samples of kernel parameters.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"plt = scatter(\n    x_train,\n    y_train;\n    xlim=(0, 1),\n    xlabel=\"x\",\n    ylabel=\"y\",\n    title=\"posterior (DynamicHMC)\",\n    label=\"Train Data\",\n)\nscatter!(plt, x_test, y_test; label=\"Test Data\")\nfor p in samples[(end - 100):end]\n    sampleplot!(plt, 0:0.02:1, gp_posterior(x_train, y_train, p))\nend\nplt","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/#Elliptical-slice-sampling","page":"One-dimensional regression","title":"Elliptical slice sampling","text":"","category":"section"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Instead of HMC, we use elliptical slice sampling which is provided by the Julia package EllipticalSliceSampling.jl.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"using EllipticalSliceSampling","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We draw 2000 samples from the posterior distribution of kernel parameters.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples = sample(ESSModel(\n    MvNormal(2, 1), # Gaussian prior\n    loglik_train,\n), ESS(), n_samples; progress=false)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We transform the samples back to the constrained space and compute the mean of both parameters:","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples_constrained = [map(softplus, p) for p in samples]\nmean_samples = mean(samples_constrained)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"2-element Vector{Float64}:\n 2.318502601073469\n 2.168334586787682","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We plot a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"histogram(\n    reduce(hcat, samples_constrained)';\n    xlabel=\"sample\",\n    ylabel=\"counts\",\n    layout=2,\n    title=[\"variance\" \"inverse length scale\"],\n)\nvline!(mean_samples'; layout=2, labels=\"mean\")","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Again we can observe that there is a significant improvement over the log-likelihood of the test data with respect to the posterior Gaussian process with default kernel parameters.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"mean(logpdf(gp_posterior(x_train, y_train, p)(x_test), y_test) for p in samples)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"-11.796598162579441","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We sample a function from the posterior GP for the final 100 samples of kernel parameters.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"plt = scatter(\n    x_train,\n    y_train;\n    xlim=(0, 1),\n    xlabel=\"x\",\n    ylabel=\"y\",\n    title=\"posterior (EllipticalSliceSampling)\",\n    label=\"Train Data\",\n)\nscatter!(plt, x_test, y_test; label=\"Test Data\")\nfor p in samples[(end - 100):end]\n    sampleplot!(plt, 0:0.02:1, gp_posterior(x_train, y_train, p))\nend\nplt","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/#Variational-Inference","page":"One-dimensional regression","title":"Variational Inference","text":"","category":"section"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Sanity check for the Evidence Lower BOund (ELBO) implemented according to M. K. Titsias's Variational learning of inducing variables in sparse Gaussian processes.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"elbo(VFE(f(rand(5))), fx, y_train)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"-25.547749075253318","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We use the LBFGS algorithm to maximize the given ELBO. It is provided by the Julia package Optim.jl.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"using Optim","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We define a function which returns the negative ELBO for different variance and inverse lengthscale parameters of the Matern kernel and different pseudo-points. We ensure that the kernel parameters are positive with the softplus function","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"f(x) = log (1 + exp x)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"and that the pseudo-points are in the unit interval 01 with the logistic function","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"f(x) = frac11 + exp(-x)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"jitter = 1e-6  # \"observing\" the latent process with some (small) amount of jitter improves numerical stability\n\nfunction objective_function(x, y)\n    function negative_elbo(params)\n        kernel =\n            softplus(params[1]) * (Matern52Kernel() ∘ ScaleTransform(softplus(params[2])))\n        f = GP(kernel)\n        fx = f(x, 0.1)\n        z = logistic.(params[3:end])\n        approx = VFE(f(z, jitter))\n        return -elbo(approx, fx, y)\n    end\n    return negative_elbo\nend","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We randomly initialize the kernel parameters and 5 pseudo points, and minimize the negative ELBO with the LBFGS algorithm and obtain the following optimal parameters:","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"x0 = rand(7)\nopt = optimize(objective_function(x_train, y_train), x0, LBFGS())","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":" * Status: success\n\n * Candidate solution\n    Final objective value:     1.086925e+01\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 1.27e-09 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.51e-10 ≰ 0.0e+00\n    |f(x) - f(x')|         = 3.41e-13 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 3.14e-14 ≰ 0.0e+00\n    |g(x)|                 = 7.62e-09 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   1  (vs limit Inf)\n    Iterations:    46\n    f(x) calls:    131\n    ∇f(x) calls:   131\n","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"opt.minimizer","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"7-element Vector{Float64}:\n  8.379380161549657\n  3.932737536558846\n  0.6917102802113171\n -1.7583267677629968\n  1.8479571186684896\n -4.133679971297448\n  1.276309736976107","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"The optimized value of the variance is","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"softplus(opt.minimizer[1])","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"8.379609687393224","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"and of the inverse lengthscale is","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"softplus(opt.minimizer[2])","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"3.9521380936435393","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We compute the log-likelihood of the test data for the resulting approximate posterior. We can observe that there is a significant improvement over the log-likelihood with the default kernel parameters of value 1.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"opt_kernel =\n    softplus(opt.minimizer[1]) *\n    (Matern52Kernel() ∘ ScaleTransform(softplus(opt.minimizer[2])))\nopt_f = GP(opt_kernel)\nopt_fx = opt_f(x_train, 0.1)\nap = posterior(VFE(opt_f(logistic.(opt.minimizer[3:end]), jitter)), opt_fx, y_train)\nlogpdf(ap(x_test), y_test)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"-1.0522111710769007","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We visualize the approximate posterior with optimized parameters.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"scatter(\n    x_train,\n    y_train;\n    xlim=(0, 1),\n    xlabel=\"x\",\n    ylabel=\"y\",\n    title=\"posterior (VI with sparse grid)\",\n    label=\"Train Data\",\n)\nscatter!(x_test, y_test; label=\"Test Data\")\nplot!(0:0.001:1, ap; label=false)\nvline!(logistic.(opt.minimizer[3:end]); label=\"Pseudo-points\")","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/#Exact-Gaussian-Process-Inference","page":"One-dimensional regression","title":"Exact Gaussian Process Inference","text":"","category":"section"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Here we use Type-II MLE to train the hyperparameters of the Gaussian process. This means that our loss function is the negative log marginal likelihood.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We re-calculate the log-likelihood of the test dataset with the default kernel parameters of value 1 for the sake of comparison.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"logpdf(p_fx(x_test), y_test)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"-232.51565975767468","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We define a function which returns the negative log marginal likelihood for different variance and inverse lengthscale parameters of the Matern kernel and different pseudo-points. We ensure that the kernel parameters are positive with the softplus function f(x) = log (1 + exp x).","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"function loss_function(x, y)\n    function negativelogmarginallikelihood(params)\n        kernel =\n            softplus(params[1]) * (Matern52Kernel() ∘ ScaleTransform(softplus(params[2])))\n        f = GP(kernel)\n        fx = f(x, 0.1)\n        return -logpdf(fx, y)\n    end\n    return negativelogmarginallikelihood\nend\n","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We randomly initialize the kernel parameters, and minimize the negative log marginal likelihood with the LBFGS algorithm and obtain the following optimal parameters:","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"θ0 = randn(2)\nopt = Optim.optimize(loss_function(x_train, y_train), θ0, LBFGS())","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":" * Status: success\n\n * Candidate solution\n    Final objective value:     1.085252e+01\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 8.65e-06 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.03e-06 ≰ 0.0e+00\n    |f(x) - f(x')|         = 2.96e-12 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 2.73e-13 ≰ 0.0e+00\n    |g(x)|                 = 4.66e-09 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    12\n    f(x) calls:    39\n    ∇f(x) calls:   39\n","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"opt.minimizer","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"2-element Vector{Float64}:\n 8.385520480737457\n 3.9687942273776353","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"The optimized value of the variance is","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"softplus(opt.minimizer[1])","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"8.385748601697454","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"and of the inverse lengthscale is","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"softplus(opt.minimizer[2])","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"3.9875141157132448","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We compute the log-likelihood of the test data for the resulting optimized posterior. We can observe that there is a significant improvement over the log-likelihood with the default kernel parameters of value 1.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"opt_kernel =\n    softplus(opt.minimizer[1]) *\n    (Matern52Kernel() ∘ ScaleTransform(softplus(opt.minimizer[2])))\n\nopt_f = GP(opt_kernel)\nopt_fx = opt_f(x_train, 0.1)\nopt_p_fx = posterior(opt_fx, y_train)\nlogpdf(opt_p_fx(x_test), y_test)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"-1.084976754998086","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We visualize the posterior with optimized parameters.","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"scatter(\n    x_train,\n    y_train;\n    xlim=(0, 1),\n    xlabel=\"x\",\n    ylabel=\"y\",\n    title=\"posterior (optimized parameters)\",\n    label=\"Train Data\",\n)\nscatter!(x_test, y_test; label=\"Test Data\")\nplot!(0:0.001:1, opt_p_fx; label=false)","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: )","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"","category":"page"},{"location":"examples/regression-1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"This page was generated using Literate.jl.","category":"page"},{"location":"concrete_features/#Features","page":"Concrete Features","title":"Features","text":"","category":"section"},{"location":"concrete_features/#Setup","page":"Concrete Features","title":"Setup","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"using AbstractGPs, Random\nrng = MersenneTwister(0)\n\n# Construct a zero-mean Gaussian process with a matern-3/2 kernel.\nf = GP(Matern32Kernel())\n\n# Specify some input and target locations.\nx = randn(rng, 10)\ny = randn(rng, 10)","category":"page"},{"location":"concrete_features/#Finite-dimensional-projection","page":"Concrete Features","title":"Finite dimensional projection","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"Look at the finite-dimensional projection of f at x, under zero-mean observation noise with variance 0.1.","category":"page"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"fx = f(x, 0.1)","category":"page"},{"location":"concrete_features/#Sample-from-GP-from-the-prior-at-x-under-noise.","page":"Concrete Features","title":"Sample from GP from the prior at x under noise.","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"y_sampled = rand(rng, fx)","category":"page"},{"location":"concrete_features/#Compute-the-log-marginal-probability-of-y.","page":"Concrete Features","title":"Compute the log marginal probability of y.","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"logpdf(fx, y)","category":"page"},{"location":"concrete_features/#Construct-the-posterior-process-implied-by-conditioning-f-at-x-on-y.","page":"Concrete Features","title":"Construct the posterior process implied by conditioning f at x on y.","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"f_posterior = posterior(fx, y)","category":"page"},{"location":"concrete_features/#A-posterior-process-follows-the-AbstractGP-interface,-so-the-same-functions-which-work-on-the-posterior-as-on-the-prior.","page":"Concrete Features","title":"A posterior process follows the AbstractGP interface, so the same functions which work on the posterior as on the prior.","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"rand(rng, f_posterior(x))\nlogpdf(f_posterior(x), y)","category":"page"},{"location":"concrete_features/#Compute-the-VFE-approximation-to-the-log-marginal-probability-of-y.","page":"Concrete Features","title":"Compute the VFE approximation to the log marginal probability of y.","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"Here, z is a set of pseudo-points. ","category":"page"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"z = randn(rng, 4)","category":"page"},{"location":"concrete_features/#Evidence-Lower-BOund-(ELBO)","page":"Concrete Features","title":"Evidence Lower BOund (ELBO)","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"We provide a ready implentation of elbo w.r.t to the pseudo points. We can perform Variational Inference on pseudo-points by maximizing the ELBO term w.r.t pseudo-points z and any kernel parameters. For more information, see examples. ","category":"page"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"elbo(VFE(f(z)), fx, y)","category":"page"},{"location":"concrete_features/#Construct-the-approximate-posterior-process-implied-by-the-VFE-approximation.","page":"Concrete Features","title":"Construct the approximate posterior process implied by the VFE approximation.","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"The optimal pseudo-points obtained above can be used to create a approximate/sparse posterior. This can be used like a regular posterior in many cases.","category":"page"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"f_approx_posterior = posterior(VFE(f(z)), fx, y)","category":"page"},{"location":"concrete_features/#An-approximate-posterior-process-is-yet-another-AbstractGP,-so-you-can-do-things-with-it-like","page":"Concrete Features","title":"An approximate posterior process is yet another AbstractGP, so you can do things with it like","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"marginals(f_approx_posterior(x))","category":"page"},{"location":"concrete_features/#Sequential-Conditioning","page":"Concrete Features","title":"Sequential Conditioning","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"Sequential conditioning allows you to compute your posterior in an online fashion. We do this in an efficient manner by updating the cholesky factorisation of the covariance matrix and avoiding recomputing it from original covariance matrix.","category":"page"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"# Define GP prior\nf = GP(SqExponentialKernel())","category":"page"},{"location":"concrete_features/#Exact-Posterior","page":"Concrete Features","title":"Exact Posterior","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"# Generate posterior with the first batch of data on the prior f1.\np_fx = posterior(f(x[1:3], 0.1), y[1:3])\n\n# Generate posterior with the second batch of data considering posterior p_fx1 as the prior.\np_p_fx = posterior(p_fx(x[4:10], 0.1), y[4:10])","category":"page"},{"location":"concrete_features/#Approximate-Posterior","page":"Concrete Features","title":"Approximate Posterior","text":"","category":"section"},{"location":"concrete_features/#Adding-observations-in-an-sequential-fashion","page":"Concrete Features","title":"Adding observations in an sequential fashion","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"Z1 = rand(rng, 4)\nZ2 = rand(rng, 3)\nZ = vcat(Z1, Z2)\np_fx1 = posterior(VFE(f(Z)), f(x[1:7], 0.1), y[1:7])\nu_p_fx1 = update_posterior(p_fx1, f(x[8:10], 0.1), y[8:10])","category":"page"},{"location":"concrete_features/#Adding-pseudo-points-in-an-sequential-fashion","page":"Concrete Features","title":"Adding pseudo-points in an sequential fashion","text":"","category":"section"},{"location":"concrete_features/","page":"Concrete Features","title":"Concrete Features","text":"p_fx2 = posterior(VFE(f(Z1)), f(x, 0.1), y)\nu_p_fx2 = update_posterior(p_fx2, f(Z2))","category":"page"},{"location":"#AbstractGPs.jl","page":"Home","title":"AbstractGPs.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Abstract types and methods for Gaussian Processes.","category":"page"},{"location":"","page":"Home","title":"Home","text":"AbstractGPs.jl is a package that defines a low-level API for working with Gaussian processes (GPs), and basic functionality for working with them in the simplest cases. While it is aimed more at developers and researchers who are interested in using it as a building block than end-users of GPs, much can be achieved with it as an end-user.","category":"page"}]
}
